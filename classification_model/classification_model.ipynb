{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4e5ab3af",
      "metadata": {
        "id": "4e5ab3af"
      },
      "source": [
        "# Classification model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9iOrfkrGug0N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dfc3413-dab2-43cc-a3ab-13980e9f3609"
      },
      "id": "9iOrfkrGug0N",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/daic_data/daic_data.zip"
      ],
      "metadata": {
        "id": "_1_t4wmiuihr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9093cba-ddcc-4d55-e7cf-2eab5a89d264"
      },
      "id": "_1_t4wmiuihr",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/daic_data/daic_data.zip\n",
            "   creating: daic_data/\n",
            "   creating: daic_data/labels/\n",
            "  inflating: daic_data/labels/dev.csv  \n",
            "  inflating: daic_data/labels/train.csv  \n",
            "  inflating: daic_data/labels/test.csv  \n",
            "   creating: daic_data/transcripts/\n",
            "  inflating: daic_data/transcripts/408_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/457_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/378_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/488_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/475_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/336_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/332_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/361_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/419_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/452_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/396_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/435_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/418_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/389_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/347_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/444_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/374_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/459_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/322_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/407_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/406_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/345_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/415_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/380_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/391_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/328_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/369_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/423_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/392_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/447_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/363_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/354_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/309_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/485_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/346_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/428_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/466_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/335_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/476_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/337_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/385_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/492_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/384_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/350_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/327_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/420_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/401_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/432_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/365_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/319_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/371_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/402_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/405_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/490_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/429_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/479_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/478_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/469_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/427_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/383_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/355_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/324_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/395_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/465_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/448_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/400_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/313_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/382_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/471_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/349_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/470_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/410_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/449_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/358_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/424_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/483_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/450_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/330_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/344_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/474_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/315_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/357_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/443_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/301_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/366_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/321_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/438_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/302_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/393_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/334_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/440_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/421_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/416_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/387_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/306_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/307_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/320_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/325_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/486_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/437_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/362_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/._487_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/412_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/326_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/351_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/439_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/311_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/372_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/430_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/303_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/489_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/417_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/305_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/375_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/314_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/368_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/341_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/364_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/399_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/381_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/356_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/463_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/425_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/331_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/480_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/338_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/451_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/422_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/352_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/434_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/390_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/433_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/308_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/379_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/404_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/403_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/442_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/484_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/454_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/329_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/340_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/370_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/373_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/467_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/472_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/318_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/312_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/377_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/491_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/461_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/462_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/411_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/445_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/413_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/487_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/431_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/464_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/436_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/360_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/441_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/473_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/456_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/458_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/397_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/409_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/348_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/333_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/386_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/446_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/323_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/477_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/339_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/376_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/468_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/317_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/481_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/482_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/426_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/310_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/300_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/453_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/359_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/388_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/455_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/367_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/304_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/316_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/343_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/414_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/353_TRANSCRIPT.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "765851fb",
      "metadata": {
        "id": "765851fb"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e81d15e0",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "e81d15e0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModel\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from peft import LoraConfig, get_peft_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"allenai/longformer-base-4096\"\n",
        "DATA_DIR = \"/content/daic_data\""
      ],
      "metadata": {
        "id": "BqvLw9mSwbDi"
      },
      "id": "BqvLw9mSwbDi",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fcd1cfe8",
      "metadata": {
        "id": "fcd1cfe8"
      },
      "source": [
        "## Data loader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_df = pd.read_csv(\"/content/augmented_dataset_400.csv\")"
      ],
      "metadata": {
        "id": "c_R11XVRh1fZ"
      },
      "id": "c_R11XVRh1fZ",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Save Augmented Dataset in Original Format\n",
        "\n",
        "def save_augmented_dataset_in_original_format(\n",
        "    augmented_df,\n",
        "    output_dir=\"./augmented_daic_data/\",\n",
        "    use_augmented=True,\n",
        "    keep_original_labels=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Save augmented dataset in the same folder and CSV structure as the original dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    import os\n",
        "    import shutil\n",
        "    import pandas as pd\n",
        "\n",
        "    # --- Create output directory structure ---\n",
        "    transcripts_dir = os.path.join(output_dir, \"transcripts\")\n",
        "    labels_dir = os.path.join(output_dir, \"labels\")\n",
        "\n",
        "    os.makedirs(transcripts_dir, exist_ok=True)\n",
        "    os.makedirs(labels_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Saving augmented dataset to: {output_dir}\")\n",
        "\n",
        "    # --- Prepare label DataFrame ---\n",
        "    label_df = pd.DataFrame(columns=[\"Participant_ID\", \"PHQ8_Binary\"])\n",
        "\n",
        "    # --- Generate transcripts by participant ---\n",
        "    participants = augmented_df['participant_id'].unique()\n",
        "    print(f\"\\nCreating transcript files for {len(participants)} participants...\")\n",
        "\n",
        "    for participant_id in participants:\n",
        "        participant_data = augmented_df[augmented_df['participant_id'] == participant_id].copy()\n",
        "\n",
        "        # First label (binary)\n",
        "        participant_label = participant_data['depression_label'].iloc[0]\n",
        "\n",
        "        # Each augmentation index = a new synthetic participant ID\n",
        "        augmentation_indexes = participant_data['augmentation_index'].unique()\n",
        "\n",
        "        for augmentation_index in augmentation_indexes:\n",
        "\n",
        "            # Subset this augmented version\n",
        "            data = participant_data[participant_data['augmentation_index'] == augmentation_index].copy()\n",
        "            data = data.sort_values(by='start_time').reset_index(drop=True)\n",
        "\n",
        "            transcript_entries = []\n",
        "\n",
        "            for idx, row in data.iterrows():\n",
        "\n",
        "                question = str(row['question']).strip()\n",
        "                answer = str(row['augmented_answer']).strip()\n",
        "\n",
        "                if not answer:\n",
        "                    continue\n",
        "\n",
        "                start_time = float(row['start_time'])\n",
        "                stop_time = start_time + 5.0  # naive duration approximation\n",
        "\n",
        "                # Ellie question entry\n",
        "                transcript_entries.append({\n",
        "                    'speaker': 'Ellie',\n",
        "                    'start_time': start_time - 2.0,\n",
        "                    'stop_time': start_time - 0.5,\n",
        "                    'value': question\n",
        "                })\n",
        "\n",
        "                # Participant answer entry\n",
        "                transcript_entries.append({\n",
        "                    'speaker': 'Participant',\n",
        "                    'start_time': start_time,\n",
        "                    'stop_time': stop_time,\n",
        "                    'value': answer\n",
        "                })\n",
        "\n",
        "            # If transcript has valid entries, save it\n",
        "            if transcript_entries:\n",
        "\n",
        "                transcript_df = pd.DataFrame(transcript_entries)\n",
        "                transcript_df = transcript_df.sort_values(by='start_time').reset_index(drop=True)\n",
        "\n",
        "                transcript_filename = f\"{int(participant_id)}{int(augmentation_index)}_TRANSCRIPT.csv\"\n",
        "                transcript_path = os.path.join(transcripts_dir, transcript_filename)\n",
        "\n",
        "                transcript_df.to_csv(transcript_path, sep='\\t', index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "                # Create new synthetic participant ID\n",
        "                new_participant_id = int(f\"{int(participant_id)}{int(augmentation_index)}\")\n",
        "\n",
        "                # Append label row\n",
        "                label_df.loc[len(label_df)] = {\n",
        "                    \"Participant_ID\": new_participant_id,\n",
        "                    \"PHQ8_Binary\": participant_label\n",
        "                }\n",
        "\n",
        "    print(f\"\\nSaved {len(participants)} participants' transcript files in:\")\n",
        "    print(f\"  → {transcripts_dir}\")\n",
        "\n",
        "    # --- Save labels ---\n",
        "    print(\"\\nMerging new labels with original ones...\")\n",
        "\n",
        "    # Caminho dos labels originais\n",
        "    old_train_labels = os.path.join(DATA_DIR, \"labels\", \"train.csv\")\n",
        "\n",
        "    # 1. Load old labels\n",
        "    if os.path.exists(old_train_labels):\n",
        "        old_df = pd.read_csv(old_train_labels)\n",
        "    else:\n",
        "        print(\"⚠️ Warning: old train.csv not found! Creating a new one.\")\n",
        "        old_df = pd.DataFrame(columns=label_df.columns)\n",
        "\n",
        "    # 2. Concatenate old + new\n",
        "    merged_labels = pd.concat([old_df, label_df], ignore_index=True)\n",
        "\n",
        "    # 3. Remove duplicates (optional but recommended)\n",
        "    merged_labels = merged_labels.drop_duplicates(subset=[\"Participant_ID\"], keep=\"first\")\n",
        "\n",
        "    # 4. Save final merged labels in the new folder\n",
        "    label_filename = \"train.csv\"\n",
        "    label_path = os.path.join(labels_dir, label_filename)\n",
        "\n",
        "    merged_labels.to_csv(label_path, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "    print(\"\\n✅ Labels merged and saved successfully!\")\n",
        "    print(f\"   Transcripts directory: {transcripts_dir}\")\n",
        "    print(f\"   Labels directory: {labels_dir}\")\n",
        "\n",
        "\n",
        "    return output_dir\n",
        "\n",
        "# Save augmented dataset in original format\n",
        "# Configuration\n",
        "\n",
        "\n",
        "save_augmented_dataset_in_original_format(\n",
        "    augmented_df,\n",
        "    output_dir=DATA_DIR,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "Hv1FzZzum6Uk",
        "outputId": "8b21c621-fa28-47c6-8a50-b8c78fbe7d27"
      },
      "id": "Hv1FzZzum6Uk",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving augmented dataset to: /content/daic_data\n",
            "\n",
            "Creating transcript files for 30 participants...\n",
            "\n",
            "Saved 30 participants' transcript files in:\n",
            "  → /content/daic_data/transcripts\n",
            "\n",
            "Merging new labels with original ones...\n",
            "\n",
            "✅ Labels merged and saved successfully!\n",
            "   Transcripts directory: /content/daic_data/transcripts\n",
            "   Labels directory: /content/daic_data/labels\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/daic_data'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ca2e6dba",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "ca2e6dba"
      },
      "outputs": [],
      "source": [
        "def process_daic_data(data_dir):\n",
        "  transcripts_dir = os.path.join(data_dir, \"transcripts\")\n",
        "  labels_dir = os.path.join(data_dir, \"labels\")\n",
        "\n",
        "  df = pd.DataFrame()\n",
        "\n",
        "  for file in os.listdir(labels_dir):\n",
        "    if not file.endswith(\".csv\"):\n",
        "      continue\n",
        "\n",
        "    split_name = file.replace(\".csv\", \"\")\n",
        "    split_df = pd.read_csv(os.path.join(labels_dir, file))\n",
        "    split_df = split_df.rename(columns={\n",
        "      \"PHQ_Binary\": \"depression_label\",\n",
        "      \"PHQ_Score\": \"depression_severity\",\n",
        "      \"PHQ8_Binary\": \"depression_label\",\n",
        "      \"PHQ8_Score\": \"depression_severity\",\n",
        "      \"Participant_ID\": \"participant_id\",\n",
        "    })\n",
        "\n",
        "    transcripts_df = create_dataframe(split_df, transcripts_dir)\n",
        "    transcripts_df[\"split\"] = split_name\n",
        "\n",
        "    df = pd.concat([df, transcripts_df], ignore_index=True)\n",
        "\n",
        "  return df\n",
        "\n",
        "def create_dataframe(split_df, transcripts_dir):\n",
        "  df = {\"text\": [], \"depression_label\": []}\n",
        "\n",
        "  for _, row in split_df.iterrows():\n",
        "    participant_id = str(int(float(row.participant_id)))\n",
        "    depression_label = int(row.depression_label)\n",
        "\n",
        "    participant_text = \"\"\n",
        "    transcript_file = os.path.join(transcripts_dir, f\"{participant_id}_TRANSCRIPT.csv\")\n",
        "    if not os.path.exists(transcript_file):\n",
        "      print(f\"Transcript file not found for participant {participant_id}\")\n",
        "      continue\n",
        "\n",
        "    transcripts = pd.read_csv(transcript_file, sep=\"\\t\")\n",
        "    participant_transcripts = transcripts[transcripts['speaker'] == 'Participant']\n",
        "\n",
        "    for _, transcript_row in participant_transcripts.iterrows():\n",
        "      participant_text += str(transcript_row.value) + \" \"\n",
        "\n",
        "    df[\"text\"].append(participant_text.strip())\n",
        "    df[\"depression_label\"].append(depression_label)\n",
        "\n",
        "  return pd.DataFrame(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cff62930",
      "metadata": {
        "id": "cff62930"
      },
      "source": [
        "## Train classification model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c1df307c",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "c1df307c"
      },
      "outputs": [],
      "source": [
        "class TranscriptsDataset(Dataset):\n",
        "  def __init__(self, dataframe, tokenizer, max_length=4096):\n",
        "    self.data = dataframe\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    text = str(self.data.iloc[idx][\"text\"])\n",
        "    label = int(self.data.iloc[idx][\"depression_label\"])\n",
        "\n",
        "    encoding = self.tokenizer(\n",
        "      text,\n",
        "      truncation=True,\n",
        "      padding=\"max_length\",\n",
        "      max_length=self.max_length,\n",
        "      return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
        "      \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
        "      \"labels\": torch.tensor(label, dtype=torch.long),\n",
        "    }\n",
        "\n",
        "class TextFeaturizer(nn.Module):\n",
        "  def __init__(self, model_name, dropout=0.5, dense_size=256,\n",
        "               lora_r=8, lora_alpha=16, lora_dropout=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    # Load Longformer encoder\n",
        "    self.encoder = AutoModel.from_pretrained(model_name)\n",
        "    hidden_size = self.encoder.config.hidden_size\n",
        "\n",
        "    self.projection = nn.Sequential(\n",
        "      nn.Linear(hidden_size, dense_size),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "      r=lora_r,\n",
        "      lora_alpha=lora_alpha,\n",
        "      target_modules=[\"query\", \"key\", \"value\"],\n",
        "      lora_dropout=lora_dropout,\n",
        "      bias=\"none\",\n",
        "      task_type=\"FEATURE_EXTRACTION\"\n",
        "    )\n",
        "    self.encoder = get_peft_model(self.encoder, lora_config)\n",
        "\n",
        "    for name, param in self.encoder.named_parameters():\n",
        "      if 'lora' not in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    cls_token = outputs.last_hidden_state[:, 0]\n",
        "    return self.projection(cls_token)\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "  \"\"\"Focal Loss for addressing class imbalance by focusing on hard examples.\"\"\"\n",
        "  def __init__(self, alpha=0.75, gamma=2.0, reduction='mean'):\n",
        "    super().__init__()\n",
        "    self.alpha = alpha\n",
        "    self.gamma = gamma\n",
        "    self.reduction = reduction\n",
        "\n",
        "  def forward(self, inputs, targets):\n",
        "    ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
        "    pt = torch.exp(-ce_loss)\n",
        "    focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "    if self.reduction == 'mean':\n",
        "      return focal_loss.mean()\n",
        "    elif self.reduction == 'sum':\n",
        "      return focal_loss.sum()\n",
        "    else:\n",
        "      return focal_loss\n",
        "\n",
        "class TextClassifier(nn.Module):\n",
        "  def __init__(self, model_name, num_labels=2, class_weights=None, use_focal_loss=True, focal_alpha=0.75, focal_gamma=2.0):\n",
        "    super().__init__()\n",
        "    self.featurizer = TextFeaturizer(model_name)\n",
        "    self.classifier = nn.Linear(256, num_labels)\n",
        "    self.use_focal_loss = use_focal_loss\n",
        "    self.focal_alpha = focal_alpha\n",
        "    self.focal_gamma = focal_gamma\n",
        "\n",
        "    # Store class weights for loss calculation (if not using focal loss)\n",
        "    if class_weights is not None and not use_focal_loss:\n",
        "      self.register_buffer('class_weights', torch.tensor(class_weights, dtype=torch.float32))\n",
        "    else:\n",
        "      self.class_weights = None\n",
        "\n",
        "    # Initialize focal loss if using it\n",
        "    if use_focal_loss:\n",
        "      self.focal_loss = FocalLoss(alpha=focal_alpha, gamma=focal_gamma)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask, labels=None):\n",
        "    features = self.featurizer(input_ids, attention_mask)\n",
        "    logits = self.classifier(features)\n",
        "\n",
        "    if labels is not None:\n",
        "      if self.use_focal_loss:\n",
        "        # Use Focal Loss for better handling of imbalanced datasets\n",
        "        loss = self.focal_loss(logits, labels)\n",
        "      else:\n",
        "        # Fallback to weighted CrossEntropyLoss if not using focal loss\n",
        "        if self.class_weights is not None:\n",
        "          loss_fn = nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "        else:\n",
        "          loss_fn = nn.CrossEntropyLoss()\n",
        "        loss = loss_fn(logits, labels)\n",
        "      return {\"loss\": loss, \"logits\": logits}\n",
        "    return {\"logits\": logits}\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "  \"\"\"Compute metrics for evaluation during training.\"\"\"\n",
        "  predictions, labels = eval_pred\n",
        "  preds = np.argmax(predictions, axis=1)\n",
        "\n",
        "  accuracy = accuracy_score(labels, preds)\n",
        "  precision = precision_score(labels, preds, zero_division=0, average='binary')\n",
        "  recall = recall_score(labels, preds, zero_division=0, average='binary')\n",
        "  f1 = f1_score(labels, preds, zero_division=0, average='binary')\n",
        "\n",
        "  return {\n",
        "    \"accuracy\": accuracy,\n",
        "    \"precision\": precision,\n",
        "    \"recall\": recall,\n",
        "    \"f1\": f1\n",
        "  }\n",
        "\n",
        "def find_optimal_threshold(trainer, val_dataset):\n",
        "  \"\"\"Find optimal threshold that maximizes F1 score on validation set, with constraints to avoid extreme predictions.\"\"\"\n",
        "  predictions = trainer.predict(val_dataset)\n",
        "  probs = torch.softmax(torch.tensor(predictions.predictions), dim=-1)\n",
        "  probs_class1 = probs[:, 1].numpy()\n",
        "  labels = predictions.label_ids\n",
        "\n",
        "  best_threshold = 0.5\n",
        "  best_f1 = 0\n",
        "  best_metrics = {}\n",
        "\n",
        "  # Try different thresholds with more granular search\n",
        "  for threshold in np.arange(0.35, 0.65, 0.01):\n",
        "    preds = (probs_class1 >= threshold).astype(int)\n",
        "\n",
        "    # Calculate metrics\n",
        "    f1 = f1_score(labels, preds, zero_division=0)\n",
        "    precision = precision_score(labels, preds, zero_division=0)\n",
        "    recall = recall_score(labels, preds, zero_division=0)\n",
        "\n",
        "    # Prefer thresholds that don't predict all as one class\n",
        "    pred_class1_count = np.sum(preds)\n",
        "    total_samples = len(preds)\n",
        "\n",
        "    # Penalize extreme thresholds (all 0s or all 1s)\n",
        "    if pred_class1_count == 0 or pred_class1_count == total_samples:\n",
        "      continue  # Skip thresholds that predict all as one class\n",
        "\n",
        "    # Use F1 score, but also consider balance\n",
        "    score = f1\n",
        "    if f1 > best_f1:\n",
        "      best_f1 = f1\n",
        "      best_threshold = threshold\n",
        "      best_metrics = {\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'pred_class1_ratio': pred_class1_count / total_samples\n",
        "      }\n",
        "\n",
        "  if best_threshold == 0.5 and best_f1 == 0:\n",
        "    # If no good threshold found, use the one that gives best balance\n",
        "    print(\"Warning: No balanced threshold found, using 0.5\")\n",
        "    best_threshold = 0.5\n",
        "    preds = (probs_class1 >= best_threshold).astype(int)\n",
        "    best_f1 = f1_score(labels, preds, zero_division=0)\n",
        "    best_metrics = {\n",
        "      'f1': best_f1,\n",
        "      'precision': precision_score(labels, preds, zero_division=0),\n",
        "      'recall': recall_score(labels, preds, zero_division=0),\n",
        "      'pred_class1_ratio': np.sum(preds) / len(preds)\n",
        "    }\n",
        "\n",
        "  print(f\"Optimal threshold: {best_threshold:.3f}\")\n",
        "  print(f\"  F1: {best_metrics.get('f1', 0):.4f}, Precision: {best_metrics.get('precision', 0):.4f}, Recall: {best_metrics.get('recall', 0):.4f}\")\n",
        "  print(f\"  Predicted class 1 ratio: {best_metrics.get('pred_class1_ratio', 0):.2%}\")\n",
        "  return best_threshold\n",
        "\n",
        "def evaluate_model(trainer, test_dataset, threshold=None):\n",
        "  from sklearn.metrics import confusion_matrix  # Import here if not already imported\n",
        "  predictions = trainer.predict(test_dataset)\n",
        "  probs = torch.softmax(torch.tensor(predictions.predictions), dim=-1)\n",
        "  probs_class1 = probs[:, 1].numpy()\n",
        "  labels = predictions.label_ids\n",
        "\n",
        "  # Use optimal threshold if provided, otherwise use argmax (threshold=0.5)\n",
        "  if threshold is not None:\n",
        "    preds = (probs_class1 >= threshold).astype(int)\n",
        "    print(f\"Using threshold: {threshold:.3f}\")\n",
        "  else:\n",
        "    preds = np.argmax(predictions.predictions, axis=1)\n",
        "    print(\"Using default threshold: 0.5 (argmax)\")\n",
        "\n",
        "  # Show first 10 predictions as examples\n",
        "  print(\"Sample predictions (first 10):\")\n",
        "  for i, (label, pred) in enumerate(zip(labels[:10], preds[:10])):\n",
        "    print(f\"  True: {label}, Predicted: {pred}, Prob(class1): {probs_class1[i]:.3f}\")\n",
        "  if len(labels) > 10:\n",
        "    print(f\"  ... ({len(labels) - 10} more predictions)\")\n",
        "\n",
        "  accuracy = accuracy_score(labels, preds)\n",
        "  precision = precision_score(labels, preds, zero_division=0)\n",
        "  recall = recall_score(labels, preds, zero_division=0)\n",
        "  f1 = f1_score(labels, preds, zero_division=0)\n",
        "\n",
        "  # Calculate and print confusion matrix\n",
        "  cm = confusion_matrix(labels, preds)\n",
        "\n",
        "  print(f\"\\nConfusion Matrix:\")\n",
        "  print(f\"                Predicted\")\n",
        "  print(f\"              Non-Dep  Depressed\")\n",
        "  print(f\"Actual Non-Dep    {cm[0,0]:4d}      {cm[0,1]:4d}\")\n",
        "  print(f\"       Depressed   {cm[1,0]:4d}      {cm[1,1]:4d}\")\n",
        "  print(f\"\\nConfusion Matrix (detailed):\")\n",
        "  print(f\"  True Negatives (TN): {cm[0,0]} - Correctly predicted non-depressed\")\n",
        "  print(f\"  False Positives (FP): {cm[0,1]} - Non-depressed predicted as depressed\")\n",
        "  print(f\"  False Negatives (FN): {cm[1,0]} - Depressed predicted as non-depressed\")\n",
        "  print(f\"  True Positives (TP): {cm[1,1]} - Correctly predicted depressed\")\n",
        "\n",
        "  print(f\"\\nTest Metrics:\")\n",
        "  print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "  print(f\"  Precision: {precision:.4f}\")\n",
        "  print(f\"  Recall: {recall:.4f}\")\n",
        "  print(f\"  F1 Score: {f1:.4f}\")\n",
        "\n",
        "  return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"confusion_matrix\": cm}\n",
        "\n",
        "def train_model(df, save_model=True, model_save_path=\"./depression_classifier_model\"):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "  train_df = df[df['split'] == 'train'].reset_index(drop=True)\n",
        "  val_df = df[df['split'] == 'dev'].reset_index(drop=True)\n",
        "  test_df = df[df['split'] == 'test'].reset_index(drop=True)\n",
        "\n",
        "  print(f\"Training samples: {len(train_df)}\")\n",
        "  print(f\"Validation samples: {len(val_df)}\")\n",
        "  print(f\"Test samples: {len(test_df)}\")\n",
        "\n",
        "  # Calculate class weights to handle imbalanced dataset (for reference, though we use Focal Loss)\n",
        "  from sklearn.utils.class_weight import compute_class_weight\n",
        "  labels = train_df['depression_label'].values\n",
        "  classes = np.unique(labels)\n",
        "  class_weights_balanced = compute_class_weight('balanced', classes=classes, y=labels)\n",
        "\n",
        "  # Apply multiplier to strengthen minority class weight (step 4)\n",
        "  weight_multiplier = 1.8  # Increase weight for minority class\n",
        "  class_weights = class_weights_balanced.copy()\n",
        "  # Find minority class (class with fewer samples)\n",
        "  class_counts = train_df['depression_label'].value_counts().sort_index()\n",
        "  minority_class = class_counts.idxmin()\n",
        "  minority_class_idx = list(classes).index(minority_class)\n",
        "  class_weights[minority_class_idx] *= weight_multiplier\n",
        "\n",
        "  class_weights_dict = dict(zip(classes, class_weights))\n",
        "  print(f\"\\nClass distribution in training set:\")\n",
        "  print(train_df['depression_label'].value_counts().sort_index())\n",
        "  print(f\"Balanced class weights: {dict(zip(classes, class_weights_balanced))}\")\n",
        "  print(f\"Adjusted class weights (multiplier={weight_multiplier}x for minority): {class_weights_dict}\")\n",
        "  print(f\"Using Focal Loss (alpha=0.85, gamma=2.5) for better imbalance handling\")\n",
        "\n",
        "  train_dataset = TranscriptsDataset(train_df, tokenizer)\n",
        "  val_dataset = TranscriptsDataset(val_df, tokenizer)\n",
        "  test_dataset = TranscriptsDataset(test_df, tokenizer)\n",
        "\n",
        "  # Use Focal Loss with adjusted parameters for better learning\n",
        "  # Higher alpha (0.85) gives more weight to minority class, higher gamma (2.5) focuses more on hard examples\n",
        "  model = TextClassifier(MODEL_NAME, num_labels=2, use_focal_loss=True, focal_alpha=0.85, focal_gamma=2.5)\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10,  # Log more frequently to see training progress\n",
        "    learning_rate=1e-5,  # Slightly lower learning rate for more stable training\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    num_train_epochs=1,  # More epochs to allow better learning\n",
        "    gradient_accumulation_steps=4,\n",
        "    fp16=True,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_f1\",  # Use F1 score instead of loss (step 5)\n",
        "    greater_is_better=True,  # F1 score should be maximized\n",
        "    warmup_steps=10,  # Add warmup for better training stability\n",
        "    weight_decay=0.01,  # Add weight decay for regularization\n",
        "  )\n",
        "\n",
        "  # training_args = TrainingArguments(\n",
        "  #   output_dir=\"./results\",\n",
        "\n",
        "  #   # --- LOGGING ---\n",
        "  #   logging_strategy=\"steps\",\n",
        "  #   logging_steps=1,                 # Log every step\n",
        "  #   report_to=\"none\",                # or \"tensorboard\"\n",
        "  #   log_level=\"info\",\n",
        "  #   log_level_replica=\"info\",\n",
        "\n",
        "  #   # --- EVAL DURING TRAINING ---\n",
        "  #   eval_strategy=\"steps\",     # Run evaluation more frequently\n",
        "  #   eval_steps=10,                   # Eval every 10 steps\n",
        "  #   save_strategy=\"steps\",           # Save more often\n",
        "  #   save_steps=10,\n",
        "\n",
        "  #   # --- TRAINING HYPERPARAMETERS ---\n",
        "  #   learning_rate=1e-5,\n",
        "  #   per_device_train_batch_size=1,\n",
        "  #   per_device_eval_batch_size=1,\n",
        "  #   gradient_accumulation_steps=4,\n",
        "  #   num_train_epochs=1,\n",
        "  #   warmup_steps=10,\n",
        "  #   weight_decay=0.01,\n",
        "  #   fp16=True,\n",
        "\n",
        "  #   # --- BEST MODEL ---\n",
        "  #   load_best_model_at_end=True,\n",
        "  #   metric_for_best_model=\"eval_f1\",\n",
        "  #   greater_is_better=True,\n",
        "\n",
        "  #   # --- MISC ---\n",
        "  #   save_total_limit=3,    # Keep only last 3 checkpoints\n",
        "  # )\n",
        "\n",
        "  trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,  # Add custom metrics (step 2)\n",
        "  )\n",
        "\n",
        "  print(\"\\nStarting training...\")\n",
        "  trainer.train()\n",
        "\n",
        "  # Find optimal threshold on validation set (step 3)\n",
        "  print(\"\\nFinding optimal threshold on validation set...\")\n",
        "\n",
        "  # First, check probability distribution on validation set\n",
        "  val_predictions = trainer.predict(val_dataset)\n",
        "  val_probs = torch.softmax(torch.tensor(val_predictions.predictions), dim=-1)\n",
        "  val_probs_class1 = val_probs[:, 1].numpy()\n",
        "  print(f\"\\nValidation set probability distribution (class 1):\")\n",
        "  print(f\"  Min: {val_probs_class1.min():.4f}, Max: {val_probs_class1.max():.4f}\")\n",
        "  print(f\"  Mean: {val_probs_class1.mean():.4f}, Std: {val_probs_class1.std():.4f}\")\n",
        "  print(f\"  Median: {np.median(val_probs_class1):.4f}\")\n",
        "\n",
        "  optimal_threshold = find_optimal_threshold(trainer, val_dataset)\n",
        "\n",
        "  print(\"\\nEvaluating on test set...\")\n",
        "  metrics = evaluate_model(trainer, test_dataset, threshold=optimal_threshold)\n",
        "\n",
        "  if save_model:\n",
        "    print(f\"\\nSaving model to {model_save_path}...\")\n",
        "    trainer.save_model(model_save_path)\n",
        "    tokenizer.save_pretrained(model_save_path)\n",
        "    print(\"Model saved successfully!\")\n",
        "\n",
        "  return trainer, metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Explore Data"
      ],
      "metadata": {
        "id": "XVQ6bjz5wMdH"
      },
      "id": "XVQ6bjz5wMdH"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "df = process_daic_data(DATA_DIR)\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(f\"Total samples: {len(df)}\")\n",
        "print(f\"\\nSplit distribution:\")\n",
        "print(df['split'].value_counts())\n",
        "print(f\"\\nLabel distribution:\")\n",
        "print(df['depression_label'].value_counts())\n",
        "print(f\"\\nLabel distribution by split:\")\n",
        "print(df.groupby(['split', 'depression_label']).size())\n",
        "\n",
        "# Display first few rows\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "uFtXt5vswO4B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "outputId": "3647f9bb-3a50-4478-949f-f8d32bacf370"
      },
      "id": "uFtXt5vswO4B",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 219\n",
            "\n",
            "Split distribution:\n",
            "split\n",
            "train    137\n",
            "test      47\n",
            "dev       35\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Label distribution:\n",
            "depression_label\n",
            "0    133\n",
            "1     86\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Label distribution by split:\n",
            "split  depression_label\n",
            "dev    0                   23\n",
            "       1                   12\n",
            "test   0                   33\n",
            "       1                   14\n",
            "train  0                   77\n",
            "       1                   60\n",
            "dtype: int64\n",
            "\n",
            "First few rows:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  depression_label  split\n",
              "0  okay how 'bout yourself here in california yea...                 0  train\n",
              "1  i'm doing good um from los angeles california ...                 0  train\n",
              "2  i'm doing alright uh originally i'm from calif...                 0  train\n",
              "3  yes it's okay <laughter> fine <laughter> i liv...                 0  train\n",
              "4  yes fine how about you here yes the weather we...                 0  train"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-82b52ac2-8214-4513-b231-a6e5eddd7056\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>depression_label</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>okay how 'bout yourself here in california yea...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i'm doing good um from los angeles california ...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i'm doing alright uh originally i'm from calif...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>yes it's okay &lt;laughter&gt; fine &lt;laughter&gt; i liv...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>yes fine how about you here yes the weather we...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-82b52ac2-8214-4513-b231-a6e5eddd7056')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-82b52ac2-8214-4513-b231-a6e5eddd7056 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-82b52ac2-8214-4513-b231-a6e5eddd7056');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5bc511fa-a841-4090-bd02-4e77fbcbc1fc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5bc511fa-a841-4090-bd02-4e77fbcbc1fc')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5bc511fa-a841-4090-bd02-4e77fbcbc1fc button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 219,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 219,\n        \"samples\": [\n          \"<synch> yes uh doing okay doing okay uh north carolina yep uh almost four years ago um i graduated college had an internship out here uh for a production company so i started doing that with a group of friends and then got work after that and enjoyed the city so much and have been working out here since yep uh probably three <th> at the most three times a year maybe twice usually the holidays maybe one random trip in there mhm um uh it's l_a's only now just starting to become home to me in the sense of that respect where like uh going home was going home now going home's like going back to see my family and seeing some friends um but um i i feel more i feel more like i belong in los angeles than i do back at home uh the food um the weather goes without saying uh i the <th> i like the resources there i like feeling um i like that uh the things that i wanna do uh are much easier much more accessible out here than they would be in north carolina so it i feel like i'm uh uh i'm able to uh pursue eh a lot of my passions ambitions in a much more functional and realistic way living in los angeles than i am in north carolina uh i mean the cost of living isn't great but at the same time too i always kind of tell people like part of the rent is having access to so much um resources <resour> so many resources out here so uh it's not even expensive as new york or boston so uh it's not the worst but um uh and also i mean it is a little bit annoying that every time i do wanna go home it's a flight not a drive i can't i mean conceivably <con> really conceivably drive home for the holidays 'cause it's about a three or four day adventure and just not practical so it's uh unfortunate that i'm so far away in that respect but i can't really change geography uh <laughter> <laughter> i studied uh history and uh media studies and production uh yes i work as a freelance writer and producer um in college i realized i was pretty good at it and uh i knew there was a way to monetize that and i came out here and uh worked a little bit uh in the uh public sector in trying to you know just doing those jobs that way and realized i could make more money have more control over my schedule and my life if i did it freelance so i've been doing it now for a little over a year now i do um i love traveling i try to uh go somewhere new for a good period of time about every uh seven to eight months just try to mix up things so i was in barcelona just recently and i'm planning on going to um either vietnam or even back to europe in next couple months so mhm uh i enjoy the break from routine i think that uh while it's nice to wake up and kind of have a vague idea where your day's going uh after a few months you can get very much kinda caught up in that and you uh pigeonhole i guess your thought process your expectations for the day and i love going to a foreign country and just waking up and knowing that everything i do that day is gonna be brand new and will be different than what i would be doing if i was in los angeles or and um our own routine so i i'm a big fan of that and i love the food so it's great to just eat um a lot of food that i love in the places where they were originally created uh yeah sure i went to uh barcelona uh like i said a few uh months ago and that was mainly because i had a friend who was living there for uh six or seven months and she kept asking me to come out and uh i finally i was at a point <p> point in work where i'd been working so much for months on end that i felt like i deserved a break so i went there for two weeks and yeah it was one of the healthiest smartest things i've done in awhile so i mm reminded me again how uh how important it is for me at least to uh to travel and to get out of the world i'm very outgoing i uh love talking to people and conversing even virtual people oop i lost ya um to relax i uh listen and play music a lot i like to read uh i like to go see movies uh cook i cook a lot so i enjoy cooking um uh my <m> my girlfriend and i are uh currently separated by distance um so i haven't seen her in weeks and that can be very uh sad obviously so i'll talk with her as much as i can but uh but yeah just uh um uh pretty simple tastes yep <participant is talking to rachel who has entered the room to fix a problem with ellie> <laughter> this is awkward isn't um i don't really have much of a temper so uh very good i guess i don't really get angry easily or quickly and uh when i do i'm usually aware of it enough that i can kinda formulate how i feel and why i'm feeling that way and what i want to be done about it so uh so it's pretty i guess i'm a pretty easygoing guy in that respect <sigh> uh honestly it was probably something petty over like uh misremembering when uh something was done or like a a date on something or like a a a fact probably wasn't anything emotionally fueled uh and it was just about yeah i think it was probably about like trying to figure out when exactly we went to san francisco and it wasn't even an argument as much as just the other person thinking the other person's misremembering so um in you know small or big situation it's very difficult for me to actually feel enraged uh i guess in the classical sense of the word um <sigh> i'm trying to think i'm not one one for regrets um um i just recently did a uh uh a project like a a film and uh the subject matter had a little bit to do with like an ex-girlfriend of mine so when it came time to release it uh i made a point i to i didn't wanna send it to her directly 'cause i didn't want her to think that i was doing to xxx her face but i did send it to her brother just because he enjoys my work and always says he wants to see stuff and i um uh maybe that <tha> she might've could've misconstrued that as being passive-aggressive i don't know um but um but yeah i mean that's something i i thought about but i don't really regret it as much as like maybe i could've just let uh everyone <ever> everyone else who i wouldn't wasn't necessarily sure about if i should do stumble upon it as opposed to me directing it to them so just my thoughts there <sigh> <laughter> hardest decision i've ever had to make <deep breath> ah um <sigh> uh it was actually probably uh probably that uh ah i don't even wasn't even a hard decision with like i said with that the the same ex-girlfriend that i mentioned of it was probably hard because uh i knew i had to uh break up with her after some stuff happened and um it was difficult 'cause i cared about her a great deal but also same tune it wasn't hard to arrive at the conclusion that i had to uh to uh break up with her but at the same time too it was not uh uh the easiest thing to kinda go through that process it's getting on a rollercoaster that you're very much aware of how mm terrible or frightening it's gonna be but you know you have to get on the rollercoaster so uh so yeah i'd say that's probably of recent memory ugh just a abundance of <laughter> reasons that uh this person was probably not the uh best person for me so uh like i said it was a it was i didn't have to uh i didn't have to weigh too much too much data to figure it out um it was pretty clear sign and i have no regrets about doing it now um i uh yeah i feel very strongly then about it and i felt i still feel very strongly that i made the right choice now um <sigh> it can be it can be tough there be <b> they'll be i feel like it's either if i if i've exhausted myself during the day between work and you know i've i've started exercising again which is a huge help but um but i'd say that it's i kind of when i get into bed at night i never know how it's gonna go sometimes it's uh sometimes i'll wake up within an hour of falling asleep and i can't go back to sleep or um i'll just yeah periodically keep getting up and up um so it's something that's gotten a little better in the last few weeks but uh yeah um like i said especially having my uh my current girlfriend uh not around like i said she's a very big source of comfort around so like that uh um but she travels a lot for work so it's something to have to adjust to and uh uh sleep can be can some that suffers from that but uh in the same respect i'm not someone who actually i feel like i don't need too much sleep i if i can get if i can get four to six hours i'm in pretty good shape that day so um uh little anxious i guess uh little more <mo> more just um uh i just whenever i get off of a assignment uh there's always just the question like what am i what's going on next and uh so far i don't have the next um project lined up so it's always that nervousness um in the past something usually does come up so you have the precedent of it usually working out but until it does you're always a little um uh anxious and it can be depressing 'cause it's like uh um you know i've chosen a very much a lifestyle that while i get to have a lot of control and autonomy over my life uh it also is all on me in order to do it and you know sometimes i'm i um i'm very jealous of the idea of people just waking up going to an office for eight hours leaving and not having to think about that work the minute they leave the office and kind of having the momentum of a group of people or or company um carrying them through when um it's a little tough whereas like me if i have an off day or if i'm a sick day um that no work gets done <laughter> you know i'm a small business that has one employee how do i cope with them um i assume you mean i guess uh people i work with how do i cope with the uh maybe the better answer would be how do i cope with that lifestyle and just i got much <m> uh much more used to it something i'm still learning and figuring out how to how to handle uh as far as not just like i know the the how rewarding the work is when i do it but it's also figuring out what's the most efficient way to live the life and you know how do i uh what's the appropriate way to budget finances what's the uh expectation i should have for myself in a given year what's realistic what's hopeful um and that's something that i'm sure will take years and years to do but i feel like as long as i'm getting better at it then uh it's progress right uh yes uh definitely once and then probably i if i would've gone back uh um i was gone in high school probably it would've been second time but um but yeah i've been diagnosed with depression once so i feel like it's one of those things that uh is something i have to keep in check throughout my entire life you know um i feel like i'm a pretty happy person for the most part but um uh has they i hate as annoying as it is but like they say like with people who are very creative or people who do a lot of stuff with the mind it tends to follow and i get that it's a self-esteem thing 'cause it's so hard to feel um so hard <har> or so easy i should <shou> suppose to get in your own head about that uh this was within a year so i probably was feeling uh symptoms having been there before it was kinda easy to figure out like ah you know it's kinda happening again so i feeling uh the symptoms again uh uh and going through those emotions and feeling like okay at first i was kinda hoping it would be something i could kind of work through myself but um i realized it was just stupid for me to try to fight the battle on my own so i went and um started seeing a therapist uh uh really bad this is when i i just couldn't sleep at all like i would get maybe an hour of sleep a night at best um i wasn't sleeping at all had just no <sigh> uh no no drive to do anything whether it was you know as simple as brushing my teeth or cleaning my apartment things like that and just being in the same time simultaneously being aware that it's like oh this is all in my head and it's not even like a a symptom so um uh yeah just a general just a general malaise that comes with depression uh no i just finished up uh finished up i mean since i didn't graduate or anything <anyth> like that but i got to a point where i felt like uh um i kind of talked through as much as i could talk through and i'd moved on eh in other places in my life it was in kind of a good good work xxx especially like i said i was i was working on something i really cared about um i'm now with someone i care about greatly and uh um there's such a potential opportunity for me to not even have to worry about the symptoms and stuff as much now that uh i felt like therapy was not necessary and also i just didn't really have the time to with as much work goes on so oh i definitely i think having someone you can confide in uh who isn't your friend or your family member who won't you know have a few drinks and tell someone what you said to them or anything like that is definitely a a something beneficial i can't imagine how anyone couldn't benefit from having that on a somewhat of a routine basis um <sigh> i don't know i'm i'm a firm <f> kind of a firm believer that the all those things like those things that kinda can haunt you are also there for a reason like uh uh my sister died when i was uh about <bout> what twelve years old and she was younger than me and watching my parents like seeing my father cry for the first time was a very kind of traumatic thing for me and it uh i felt like i grew up a lot that day so it's um also difficult to see that and see how my parents had to deal with losing a child i don't want to forget those things 'cause i think they're very integral to um how i cope with life and how i perceive uh um hardships and obstacles and uh having that image of knowing my father on it's very very humanizing 'cause it's so easy to kind of you idolize your parents and i still idolize my father but to know he's he's a guy like me and as i get older it's um easier identify with uh the man he is <psh> so i'd be uh sixteen or six um uh i kind of like the notion uh that uh they're they always say you wouldn't worry as much what people thought about you if you realized how people how much people didn't think about you and that's really hard to tell a sixteen year old and i don't know what the point of telling a six year old but um but yeah i feel like especially when you're in high school until it gets i mean nowadays you <y> i mean we live in los angeles where so much is caught up in how people think about you and i still am very if i find out someone eh either is upset with me or doesn't like me it bothers me to a great deal or i hate letting people down so i would like to come to a healthier place where just knowing that sometimes you just can't really control how people feel about you even if you have the best intentions in the world so uh i'm a very compassionate guy i um i'm there for my friends i'm pretty selfless in that respect i um have a good humor about things like even with <laughter> even with being depressed or depression i can joke about it it's not you know i'm not a morbid depressed more just the i'm aware of when i'm uh i'm low and uh i treat it as such and i don't you know i don't try to deny it i don't try to like any of that but i i i have a i'm a i'm a pretty like practical person in that respect i don't think uh even when i'm depressed i don't take it out on people or i don't think people people have told me they don't even seem to when i mention that <cough> been going through like depression no one seems to like or they don't mention i for one feel like when i do go through it it's like oh everyone can just tell and i'm just a smoking gun of sadness basically <laughter> but uh but no i i tend to carry it pretty well and uh um i don't i don't lose sight of uh the reality <rea> the reality of what i know is happening so mm um i was on a my uh my girlfriend came uh she was in thailand for three weeks and then she was supposed to go to spain for two but she canceled that trip and came back and stayed with me for about ten days and um yeah i know i <laughter> i love her she's great um she's the coolest person i know and so much fun and she uh when she's around she makes me so happy and uh um so having her for ten days and we just had like normal we we rarely get normal couples time like it's always like oh i see you for five days and then one of us has to go somewhere um so to have ten days just normal where we could just cook dinner at home like lie in bed and read books or anything like that was great and uh it makes me so happy that i have someone that i can do that with and just it feels so uh practical and feels so right um <sigh> i guess for xxx i'm i'm i'm a good guy like i i i know i don't uh i don't think i screw people over i don't um i'm a good friend to have i'm i'm there for people i look out for people i care uh about people and what they think of me and that goes especially much for my friends and i try to make sure you know with a lot of my friends being on the east coast i try to carve out time to make sure that they know i think about them and i call them and so i i feel like uh for a lot of people who count me as their friend i'm probably one of their better ones in just the sense that i'm i know how much i appreciate and respect uh the friendships we have thanks ellie bye\",\n          \"<synch> yes i am i am pretty good uh i was born in los angeles california yes um i like the fact that everything is close by um all the stores the shops xxx um the traffic <laughter> um i've traveled but not too much uh going away from your usual days um exploring new sights um three weeks no a month ago i went to las vegas uh with <wit> me and my friends and we just went sightseeing um in las vegas we went to the hoover dam and um it was breathtaking um the it was really big and we saw the the big canyon and it was impressive i've never seen something like that before i studied nursing uh i graduated two months ago so i took my uh board exam to be licensed and i passed so i'm just waiting for my license in the mail my dream job is to be a registered nurse um long time ago when i was a little boy i got injured and my parent had to take me to the hospital and um the doctors came by and they had to stitch me up on the leg and i saw all of that and that made me want to pursue it the medical field uh yes i am very happy uh my relationship with my family is okay not good not bad it's it it's in between <laughter> uh i am close i am very close to them um mm if i have a problem i i go to them for help um someone who's been a positive influence has been my best friend since elementary school yes yes uh we've known each other since we were in the third grade and we've uh we've gone to elementary middle school and high school together and he lives like two blocks away so he's basically like like a brother of <o> of mine i am shy um i really don't know i've i've always been a quiet person um ever since i can remember hmm what do i do i i go into my room i turn my a_c on and i just relax in my bed <laughter> i am pretty good i'm pretty good at controlling my temper yes the last time i argued with someone was about um work i was disappointed um not angry just disappointed because my manager he wanted me to do something faster than what i usually can handle and he already knew that um my workload was a lot but he didn't understand that so that made me just very like sad mm i cannot remember one the hardest decision i ever had to make was dropping out of school um i felt that i didn't really belong uh far away from home and so i was just thinking ahead um i was in a major that i didn't really like and so i decided to just not go next semester and go back home to l_a um right now that i'm looking at it yes i i am very happy that i did that because i wouldn't have gone to nursing school and i wouldn't have been a licensed vocational nurse right now hmm i don't have an event um it depends on the days um where i work or where i don't work yes right now i i work at um at a retail store early in the morning i i go at three in the morning and i get out at ten a_m so it's really hard for me to get a good night's sleep when i don't sleep well i can be a little bit grumpy uh irritated or just frustrated with the situation lately i've been okay <o> okay i've been good uh no i have not no no hmm the last time i was really really happy and excited was when i found out that i passed my license board exam to be a nurse <laughter> my best friend would've described <des> would describe me as a caring nice sweet shy quiet guy um physically speaking i would change how i look from the outside um i i'm a little <l> i'm overweight and i would like to lose some weight and just look good from the outside hmm i don't have one no i don't um this one time i <sigh> at my work uh it's a retail store i i kinda took some merchandise without buying it um it was a risk that i took but um since no one was around and the opportunity <opp> the opportunity was right there for me to get it i just got it and i took it home i don't know yes last weekend i went to i i went yard saleing with my family and it was pretty fun and going out to different yard sales and finding uh random stuff um i bought this big black bean bag it was huge and it was on it was only for two dollars and so i got it and now it's in my room and i sit on it <laughter> i am most proud of is the fact that i am a nurse um that i am a nurse um i'm just waiting for my for my license in the mail and after that i would just send out my resumes to different hospitals and xxx and nursing homes bye\",\n          \"<synch> yes i'm doing well thank you san luis obispo yes four years ago about once a month small quiet fresh air less people it's nice i went to school in los angeles <laughter> i studied music industry yes i work in the music industry it is a passion of mine and i got into school and went with it how hard is what <laughter> i don't really have a dream job i don't know if that exists actually it's probably changing all the time <laughter> depends on the situation sometimes i'm shy and sometimes i'm outgoing well i can't see your shoes but i like to travel yes meeting new people seeing new things exploring the landscape uh i went to alaska on a fishing trip on that trip um catching an octopus i was deep sea fishing and for halibut and rather than catching a halibut i lured in a octopus very <laughter> <laughter> i play golf i swim i read i'm pretty calm um probably couple days ago but it was and it was about a musical project that i was involved in i felt heated <laughter> i'm not quite sure i'm not quite sure i'd have to think about that for a while i'm not really sure um i had a bad performance one time and i wish people who i knew didn't see it because we weren't well rehearsed and i was embarrassed afterwards i have a brother and parents who are divorced i'm pretty close i see them regularly talk openly about most subjects my brother what do you wanna know about my brother i'm not really sure most of the time it's pretty easy um i drink more coffee to compensate fine no what does p_t_s_d stand for no i haven't um i'm happy all the time <laughter> um ambitious a go-getter feisty i'm not quite sure i can't recall right now uh i went to a concert saw live music drank good beer yeah <laughter> i'm a good listener i'm passionate and i'm alert ever in regards to what no i don't i don't i normally don't regret things because there's too much else to live for to think about the past do you <laughter> okay <deep breath> what was that question to read more and watch t_v watch the computer screen less because that makes you have attention deficit disorder and you focus less on things that are in front of you in real life i know <laughter> i exercise i sleep in late i go golfing from time to time i go to concerts i go to bars hang out with friends at the beach it depends what am i most proud of um i'm healthy and i understand the world around me and i'm passionate about things that i'm doing and i have goals that i've set that i'm working towards yeah <laughter> what was your name that's right goodbye\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"depression_label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"split\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"train\",\n          \"test\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Model"
      ],
      "metadata": {
        "id": "MFIEwHBRwow_"
      },
      "id": "MFIEwHBRwow_"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer, metrics = train_model(df, save_model=True, model_save_path=\"./depression_classifier_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0GiMcfirwl-u",
        "outputId": "2587b989-3c44-4e44-e158-a8cbba14563f"
      },
      "id": "0GiMcfirwl-u",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 137\n",
            "Validation samples: 35\n",
            "Test samples: 47\n",
            "\n",
            "Class distribution in training set:\n",
            "depression_label\n",
            "0    77\n",
            "1    60\n",
            "Name: count, dtype: int64\n",
            "Balanced class weights: {np.int64(0): np.float64(0.8896103896103896), np.int64(1): np.float64(1.1416666666666666)}\n",
            "Adjusted class weights (multiplier=1.8x for minority): {np.int64(0): np.float64(0.8896103896103896), np.int64(1): np.float64(2.055)}\n",
            "Using Focal Loss (alpha=0.85, gamma=2.5) for better imbalance handling\n",
            "\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgabrielfreddi\u001b[0m (\u001b[33mfreddi\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.23.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251125_015442-x07nwtys</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/freddi/huggingface/runs/x07nwtys' target=\"_blank\">ruby-wood-18</a></strong> to <a href='https://wandb.ai/freddi/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/freddi/huggingface' target=\"_blank\">https://wandb.ai/freddi/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/freddi/huggingface/runs/x07nwtys' target=\"_blank\">https://wandb.ai/freddi/huggingface/runs/x07nwtys</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [35/35 01:45, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.121900</td>\n",
              "      <td>0.109686</td>\n",
              "      <td>0.342857</td>\n",
              "      <td>0.342857</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.510638</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Finding optimal threshold on validation set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation set probability distribution (class 1):\n",
            "  Min: 0.5138, Max: 0.5201\n",
            "  Mean: 0.5174, Std: 0.0016\n",
            "  Median: 0.5176\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No balanced threshold found, using 0.5\n",
            "Optimal threshold: 0.500\n",
            "  F1: 0.5106, Precision: 0.3429, Recall: 1.0000\n",
            "  Predicted class 1 ratio: 100.00%\n",
            "\n",
            "Evaluating on test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using threshold: 0.500\n",
            "Sample predictions (first 10):\n",
            "  True: 0, Predicted: 1, Prob(class1): 0.520\n",
            "  True: 0, Predicted: 1, Prob(class1): 0.518\n",
            "  True: 0, Predicted: 1, Prob(class1): 0.518\n",
            "  True: 1, Predicted: 1, Prob(class1): 0.521\n",
            "  True: 1, Predicted: 1, Prob(class1): 0.517\n",
            "  True: 1, Predicted: 1, Prob(class1): 0.516\n",
            "  True: 0, Predicted: 1, Prob(class1): 0.519\n",
            "  True: 0, Predicted: 1, Prob(class1): 0.521\n",
            "  True: 0, Predicted: 1, Prob(class1): 0.519\n",
            "  True: 1, Predicted: 1, Prob(class1): 0.515\n",
            "  ... (37 more predictions)\n",
            "\n",
            "Confusion Matrix:\n",
            "                Predicted\n",
            "              Non-Dep  Depressed\n",
            "Actual Non-Dep       0        33\n",
            "       Depressed      0        14\n",
            "\n",
            "Confusion Matrix (detailed):\n",
            "  True Negatives (TN): 0 - Correctly predicted non-depressed\n",
            "  False Positives (FP): 33 - Non-depressed predicted as depressed\n",
            "  False Negatives (FN): 0 - Depressed predicted as non-depressed\n",
            "  True Positives (TP): 14 - Correctly predicted depressed\n",
            "\n",
            "Test Metrics:\n",
            "  Accuracy: 0.2979\n",
            "  Precision: 0.2979\n",
            "  Recall: 1.0000\n",
            "  F1 Score: 0.4590\n",
            "\n",
            "Saving model to ./depression_classifier_model...\n",
            "Model saved successfully!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}