{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4e5ab3af",
      "metadata": {
        "id": "4e5ab3af"
      },
      "source": [
        "# Classification model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "9iOrfkrGug0N",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iOrfkrGug0N",
        "outputId": "b59de652-9124-4ff7-f9d3-2859bd13321c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "_1_t4wmiuihr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1_t4wmiuihr",
        "outputId": "315031f1-be0f-4a91-e4fe-c111a1d167dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/daic_data/daic_data.zip\n",
            "   creating: daic_data/\n",
            "   creating: daic_data/labels/\n",
            "  inflating: daic_data/labels/dev.csv  \n",
            "  inflating: daic_data/labels/train.csv  \n",
            "  inflating: daic_data/labels/test.csv  \n",
            "   creating: daic_data/transcripts/\n",
            "  inflating: daic_data/transcripts/408_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/457_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/378_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/488_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/475_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/336_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/332_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/361_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/419_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/452_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/396_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/435_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/418_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/389_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/347_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/444_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/374_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/459_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/322_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/407_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/406_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/345_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/415_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/380_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/391_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/328_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/369_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/423_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/392_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/447_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/363_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/354_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/309_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/485_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/346_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/428_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/466_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/335_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/476_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/337_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/385_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/492_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/384_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/350_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/327_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/420_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/401_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/432_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/365_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/319_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/371_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/402_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/405_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/490_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/429_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/479_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/478_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/469_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/427_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/383_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/355_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/324_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/395_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/465_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/448_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/400_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/313_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/382_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/471_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/349_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/470_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/410_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/449_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/358_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/424_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/483_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/450_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/330_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/344_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/474_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/315_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/357_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/443_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/301_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/366_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/321_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/438_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/302_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/393_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/334_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/440_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/421_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/416_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/387_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/306_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/307_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/320_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/325_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/486_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/437_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/362_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/._487_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/412_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/326_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/351_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/439_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/311_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/372_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/430_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/303_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/489_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/417_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/305_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/375_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/314_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/368_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/341_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/364_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/399_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/381_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/356_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/463_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/425_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/331_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/480_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/338_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/451_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/422_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/352_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/434_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/390_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/433_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/308_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/379_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/404_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/403_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/442_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/484_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/454_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/329_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/340_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/370_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/373_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/467_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/472_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/318_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/312_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/377_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/491_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/461_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/462_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/411_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/445_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/413_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/487_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/431_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/464_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/436_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/360_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/441_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/473_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/456_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/458_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/397_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/409_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/348_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/333_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/386_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/446_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/323_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/477_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/339_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/376_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/468_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/317_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/481_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/482_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/426_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/310_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/300_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/453_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/359_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/388_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/455_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/367_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/304_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/316_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/343_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/414_TRANSCRIPT.csv  \n",
            "  inflating: daic_data/transcripts/353_TRANSCRIPT.csv  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/drive/MyDrive/daic_data/daic_data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "765851fb",
      "metadata": {
        "id": "765851fb"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "e81d15e0",
      "metadata": {
        "id": "e81d15e0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModel, EarlyStoppingCallback\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from peft import LoraConfig, get_peft_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "BqvLw9mSwbDi",
      "metadata": {
        "id": "BqvLw9mSwbDi"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"allenai/longformer-base-4096\"\n",
        "DATA_DIR = \"/content/daic_data\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcd1cfe8",
      "metadata": {
        "id": "fcd1cfe8"
      },
      "source": [
        "## Data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "c_R11XVRh1fZ",
      "metadata": {
        "id": "c_R11XVRh1fZ"
      },
      "outputs": [],
      "source": [
        "augmented_df = pd.read_csv(\"/content/augmented_dataset_745.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "Hv1FzZzum6Uk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "Hv1FzZzum6Uk",
        "outputId": "78c449e6-92a4-4ef1-e67a-a7b45e26d659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving augmented dataset to: /content/daic_data\n",
            "\n",
            "Creating transcript files for 30 participants...\n",
            "\n",
            "Saved 30 participants' transcript files in:\n",
            "  → /content/daic_data/transcripts\n",
            "\n",
            "Merging new labels with original ones...\n",
            "\n",
            "✅ Labels merged and saved successfully!\n",
            "   Transcripts directory: /content/daic_data/transcripts\n",
            "   Labels directory: /content/daic_data/labels\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/daic_data'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "## Save Augmented Dataset in Original Format\n",
        "\n",
        "def save_augmented_dataset_in_original_format(\n",
        "    augmented_df,\n",
        "    output_dir=\"./augmented_daic_data/\",\n",
        "    use_augmented=True,\n",
        "    keep_original_labels=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Save augmented dataset in the same folder and CSV structure as the original dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    import os\n",
        "    import shutil\n",
        "    import pandas as pd\n",
        "\n",
        "    # --- Create output directory structure ---\n",
        "    transcripts_dir = os.path.join(output_dir, \"transcripts\")\n",
        "    labels_dir = os.path.join(output_dir, \"labels\")\n",
        "\n",
        "    os.makedirs(transcripts_dir, exist_ok=True)\n",
        "    os.makedirs(labels_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Saving augmented dataset to: {output_dir}\")\n",
        "\n",
        "    # --- Prepare label DataFrame ---\n",
        "    label_df = pd.DataFrame(columns=[\"Participant_ID\", \"PHQ8_Binary\"])\n",
        "\n",
        "    # --- Generate transcripts by participant ---\n",
        "    participants = augmented_df['participant_id'].unique()\n",
        "    print(f\"\\nCreating transcript files for {len(participants)} participants...\")\n",
        "\n",
        "    for participant_id in participants:\n",
        "        participant_data = augmented_df[augmented_df['participant_id'] == participant_id].copy()\n",
        "\n",
        "        # First label (binary)\n",
        "        participant_label = participant_data['depression_label'].iloc[0]\n",
        "\n",
        "        # Each augmentation index = a new synthetic participant ID\n",
        "        augmentation_indexes = participant_data['augmentation_index'].unique()\n",
        "\n",
        "        for augmentation_index in augmentation_indexes:\n",
        "\n",
        "            # Subset this augmented version\n",
        "            data = participant_data[participant_data['augmentation_index'] == augmentation_index].copy()\n",
        "            data = data.sort_values(by='start_time').reset_index(drop=True)\n",
        "\n",
        "            transcript_entries = []\n",
        "\n",
        "            for idx, row in data.iterrows():\n",
        "\n",
        "                question = str(row['question']).strip()\n",
        "                answer = str(row['augmented_answer']).strip()\n",
        "\n",
        "                if not answer:\n",
        "                    continue\n",
        "\n",
        "                start_time = float(row['start_time'])\n",
        "                stop_time = start_time + 5.0  # naive duration approximation\n",
        "\n",
        "                # Ellie question entry\n",
        "                transcript_entries.append({\n",
        "                    'speaker': 'Ellie',\n",
        "                    'start_time': start_time - 2.0,\n",
        "                    'stop_time': start_time - 0.5,\n",
        "                    'value': question\n",
        "                })\n",
        "\n",
        "                # Participant answer entry\n",
        "                transcript_entries.append({\n",
        "                    'speaker': 'Participant',\n",
        "                    'start_time': start_time,\n",
        "                    'stop_time': stop_time,\n",
        "                    'value': answer\n",
        "                })\n",
        "\n",
        "            # If transcript has valid entries, save it\n",
        "            if transcript_entries:\n",
        "\n",
        "                transcript_df = pd.DataFrame(transcript_entries)\n",
        "                transcript_df = transcript_df.sort_values(by='start_time').reset_index(drop=True)\n",
        "\n",
        "                transcript_filename = f\"{int(participant_id)}{int(augmentation_index)}_TRANSCRIPT.csv\"\n",
        "                transcript_path = os.path.join(transcripts_dir, transcript_filename)\n",
        "\n",
        "                transcript_df.to_csv(transcript_path, sep='\\t', index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "                # Create new synthetic participant ID\n",
        "                new_participant_id = int(f\"{int(participant_id)}{int(augmentation_index)}\")\n",
        "\n",
        "                # Append label row\n",
        "                label_df.loc[len(label_df)] = {\n",
        "                    \"Participant_ID\": new_participant_id,\n",
        "                    \"PHQ8_Binary\": participant_label\n",
        "                }\n",
        "\n",
        "    print(f\"\\nSaved {len(participants)} participants' transcript files in:\")\n",
        "    print(f\"  → {transcripts_dir}\")\n",
        "\n",
        "    # --- Save labels ---\n",
        "    print(\"\\nMerging new labels with original ones...\")\n",
        "\n",
        "    # Caminho dos labels originais\n",
        "    old_train_labels = os.path.join(DATA_DIR, \"labels\", \"train.csv\")\n",
        "\n",
        "    # 1. Load old labels\n",
        "    if os.path.exists(old_train_labels):\n",
        "        old_df = pd.read_csv(old_train_labels)\n",
        "    else:\n",
        "        print(\"⚠️ Warning: old train.csv not found! Creating a new one.\")\n",
        "        old_df = pd.DataFrame(columns=label_df.columns)\n",
        "\n",
        "    # 2. Concatenate old + new\n",
        "    merged_labels = pd.concat([old_df, label_df], ignore_index=True)\n",
        "\n",
        "    # 3. Remove duplicates (optional but recommended)\n",
        "    merged_labels = merged_labels.drop_duplicates(subset=[\"Participant_ID\"], keep=\"first\")\n",
        "\n",
        "    # 4. Save final merged labels in the new folder\n",
        "    label_filename = \"train.csv\"\n",
        "    label_path = os.path.join(labels_dir, label_filename)\n",
        "\n",
        "    merged_labels.to_csv(label_path, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "    print(\"\\n✅ Labels merged and saved successfully!\")\n",
        "    print(f\"   Transcripts directory: {transcripts_dir}\")\n",
        "    print(f\"   Labels directory: {labels_dir}\")\n",
        "\n",
        "\n",
        "    return output_dir\n",
        "\n",
        "# Save augmented dataset in original format\n",
        "# Configuration\n",
        "\n",
        "\n",
        "save_augmented_dataset_in_original_format(\n",
        "    augmented_df,\n",
        "    output_dir=DATA_DIR,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "ca2e6dba",
      "metadata": {
        "id": "ca2e6dba"
      },
      "outputs": [],
      "source": [
        "def process_daic_data(data_dir):\n",
        "  transcripts_dir = os.path.join(data_dir, \"topic_transcripts/transcripts\")\n",
        "  # transcripts_dir = os.path.join(data_dir, \"transcripts\")\n",
        "  labels_dir = os.path.join(data_dir, \"labels\")\n",
        "\n",
        "  df = pd.DataFrame()\n",
        "\n",
        "  for file in os.listdir(labels_dir):\n",
        "    if not file.endswith(\".csv\"):\n",
        "      continue\n",
        "\n",
        "    split_name = file.replace(\".csv\", \"\")\n",
        "    split_df = pd.read_csv(os.path.join(labels_dir, file))\n",
        "    split_df = split_df.rename(columns={\n",
        "      \"PHQ_Binary\": \"depression_label\",\n",
        "      \"PHQ_Score\": \"depression_severity\",\n",
        "      \"PHQ8_Binary\": \"depression_label\",\n",
        "      \"PHQ8_Score\": \"depression_severity\",\n",
        "      \"Participant_ID\": \"participant_id\",\n",
        "    })\n",
        "\n",
        "    transcripts_df = create_dataframe(split_df, transcripts_dir)\n",
        "    transcripts_df[\"split\"] = split_name\n",
        "\n",
        "    df = pd.concat([df, transcripts_df], ignore_index=True)\n",
        "\n",
        "  return df\n",
        "\n",
        "def create_dataframe(split_df, transcripts_dir):\n",
        "  df = {\"text\": [], \"depression_label\": []}\n",
        "\n",
        "  for _, row in split_df.iterrows():\n",
        "    participant_id = str(int(float(row.participant_id)))\n",
        "    depression_label = int(row.depression_label)\n",
        "\n",
        "    participant_text = \"\"\n",
        "    transcript_file = os.path.join(transcripts_dir, f\"{participant_id}_TRANSCRIPT.csv\")\n",
        "    if not os.path.exists(transcript_file):\n",
        "      print(f\"Transcript file not found for participant {participant_id}\")\n",
        "      continue\n",
        "\n",
        "    transcripts = pd.read_csv(transcript_file, sep=\"\\t\")\n",
        "    participant_transcripts = transcripts[transcripts['speaker'] == 'Participant']\n",
        "\n",
        "    for _, transcript_row in participant_transcripts.iterrows():\n",
        "      participant_text += str(transcript_row.value) + \" \"\n",
        "\n",
        "    df[\"text\"].append(participant_text.strip())\n",
        "    df[\"depression_label\"].append(depression_label)\n",
        "\n",
        "  return pd.DataFrame(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cff62930",
      "metadata": {
        "id": "cff62930"
      },
      "source": [
        "## Train classification model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "c1df307c",
      "metadata": {
        "id": "c1df307c"
      },
      "outputs": [],
      "source": [
        "class TranscriptsDataset(Dataset):\n",
        "  def __init__(self, dataframe, tokenizer, max_length=4096):\n",
        "    self.data = dataframe\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    text = str(self.data.iloc[idx][\"text\"])\n",
        "    label = int(self.data.iloc[idx][\"depression_label\"])\n",
        "\n",
        "    encoding = self.tokenizer(\n",
        "      text,\n",
        "      truncation=True,\n",
        "      padding=\"max_length\",\n",
        "      max_length=self.max_length,\n",
        "      return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
        "      \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
        "      \"labels\": torch.tensor(label, dtype=torch.long),\n",
        "    }\n",
        "\n",
        "class TextFeaturizer(nn.Module):\n",
        "  def __init__(self, model_name, dropout=0.5, dense_size=256,\n",
        "               lora_r=8, lora_alpha=16, lora_dropout=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    # Load Longformer encoder\n",
        "    self.encoder = AutoModel.from_pretrained(model_name)\n",
        "    hidden_size = self.encoder.config.hidden_size\n",
        "\n",
        "    self.projection = nn.Sequential(\n",
        "      nn.Linear(hidden_size, dense_size),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "      r=lora_r,\n",
        "      lora_alpha=lora_alpha,\n",
        "      target_modules=[\"query\", \"key\", \"value\"],\n",
        "      lora_dropout=lora_dropout,\n",
        "      bias=\"none\",\n",
        "      task_type=\"FEATURE_EXTRACTION\"\n",
        "    )\n",
        "    self.encoder = get_peft_model(self.encoder, lora_config)\n",
        "\n",
        "    for name, param in self.encoder.named_parameters():\n",
        "      if 'lora' not in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    cls_token = outputs.last_hidden_state[:, 0]\n",
        "    return self.projection(cls_token)\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "  \"\"\"Focal Loss for addressing class imbalance by focusing on hard examples.\"\"\"\n",
        "  def __init__(self, alpha=0.75, gamma=2.0, reduction='mean'):\n",
        "    super().__init__()\n",
        "    self.alpha = alpha\n",
        "    self.gamma = gamma\n",
        "    self.reduction = reduction\n",
        "\n",
        "  def forward(self, inputs, targets):\n",
        "    ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
        "    pt = torch.exp(-ce_loss)\n",
        "    focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "    if self.reduction == 'mean':\n",
        "      return focal_loss.mean()\n",
        "    elif self.reduction == 'sum':\n",
        "      return focal_loss.sum()\n",
        "    else:\n",
        "      return focal_loss\n",
        "\n",
        "class TextClassifier(nn.Module):\n",
        "  def __init__(self, model_name, num_labels=2, class_weights=None, use_focal_loss=True, focal_alpha=0.75, focal_gamma=2.0):\n",
        "    super().__init__()\n",
        "    self.featurizer = TextFeaturizer(model_name)\n",
        "    self.classifier = nn.Linear(256, num_labels)\n",
        "    self.use_focal_loss = use_focal_loss\n",
        "    self.focal_alpha = focal_alpha\n",
        "    self.focal_gamma = focal_gamma\n",
        "\n",
        "    # Store class weights for loss calculation (if not using focal loss)\n",
        "    if class_weights is not None and not use_focal_loss:\n",
        "      self.register_buffer('class_weights', torch.tensor(class_weights, dtype=torch.float32))\n",
        "    else:\n",
        "      self.class_weights = None\n",
        "\n",
        "    # Initialize focal loss if using it\n",
        "    if use_focal_loss:\n",
        "      self.focal_loss = FocalLoss(alpha=focal_alpha, gamma=focal_gamma)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask, labels=None):\n",
        "    features = self.featurizer(input_ids, attention_mask)\n",
        "    logits = self.classifier(features)\n",
        "\n",
        "    # Add regularization to prevent extreme logits (helps prevent all-one-class predictions)\n",
        "    # Penalize large logit differences between classes to encourage more balanced predictions\n",
        "    logit_diff = torch.abs(logits[:, 0] - logits[:, 1])\n",
        "    logit_regularization = 0.01 * torch.mean(logit_diff ** 2)  # Penalize large differences\n",
        "    # Also add L2 regularization on logits to keep them from becoming too extreme\n",
        "    logit_l2 = 0.001 * torch.mean(logits ** 2)\n",
        "    total_regularization = logit_regularization + logit_l2\n",
        "\n",
        "    if labels is not None:\n",
        "      if self.use_focal_loss:\n",
        "        # Use Focal Loss for better handling of imbalanced datasets\n",
        "        loss = self.focal_loss(logits, labels)\n",
        "      else:\n",
        "        # Fallback to weighted CrossEntropyLoss if not using focal loss\n",
        "        if self.class_weights is not None:\n",
        "          loss_fn = nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "        else:\n",
        "          loss_fn = nn.CrossEntropyLoss()\n",
        "        loss = loss_fn(logits, labels)\n",
        "\n",
        "      # Add regularization to loss\n",
        "      loss = loss + total_regularization\n",
        "      return {\"loss\": loss, \"logits\": logits}\n",
        "    return {\"logits\": logits}\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "  \"\"\"Compute metrics for evaluation during training with balanced predictions.\n",
        "  Uses adaptive threshold to prevent all-one-class predictions.\"\"\"\n",
        "  predictions, labels = eval_pred\n",
        "\n",
        "  # Convert logits to probabilities\n",
        "  probs = torch.softmax(torch.tensor(predictions), dim=-1).numpy()\n",
        "  probs_class1 = probs[:, 1]\n",
        "\n",
        "  # Calculate true class distribution\n",
        "  class_1_ratio = np.mean(labels)\n",
        "\n",
        "  # Strategy 1: Try to match the class distribution with quantile threshold\n",
        "  if class_1_ratio > 0.05 and class_1_ratio < 0.95:\n",
        "    threshold = np.percentile(probs_class1, (1 - class_1_ratio) * 100)\n",
        "    preds = (probs_class1 >= threshold).astype(int)\n",
        "  else:\n",
        "    # For extreme distributions, start with median\n",
        "    threshold = np.median(probs_class1)\n",
        "    preds = (probs_class1 >= threshold).astype(int)\n",
        "\n",
        "  # Strategy 2: Ensure we have predictions for both classes\n",
        "  unique_preds = np.unique(preds)\n",
        "  if len(unique_preds) == 1:\n",
        "    # If all predictions are the same class, find a threshold that gives diversity\n",
        "    # Try a range of thresholds to find one that predicts both classes\n",
        "    best_threshold = 0.5\n",
        "    best_f1 = -1\n",
        "\n",
        "    for trial_threshold in np.arange(0.1, 0.9, 0.05):\n",
        "      preds_trial = (probs_class1 >= trial_threshold).astype(int)\n",
        "      unique_trial = np.unique(preds_trial)\n",
        "\n",
        "      # Prefer thresholds that predict both classes\n",
        "      if len(unique_trial) > 1:\n",
        "        # Calculate F1 with this threshold\n",
        "        f1_trial = f1_score(labels, preds_trial, zero_division=0)\n",
        "        if f1_trial > best_f1:\n",
        "          best_f1 = f1_trial\n",
        "          best_threshold = trial_threshold\n",
        "          preds = preds_trial\n",
        "\n",
        "    # If still all one class after trying thresholds, force some diversity\n",
        "    if len(np.unique(preds)) == 1:\n",
        "      # Sort probabilities and predict top k% as class 1\n",
        "      k = max(10, int(class_1_ratio * 100))  # At least 10% or match distribution\n",
        "      k = min(90, k)  # At most 90%\n",
        "      sorted_indices = np.argsort(probs_class1)\n",
        "      preds = np.zeros_like(labels)\n",
        "      preds[sorted_indices[-k:]] = 1\n",
        "\n",
        "  accuracy = accuracy_score(labels, preds)\n",
        "  precision = precision_score(labels, preds, zero_division=0, average='binary')\n",
        "  recall = recall_score(labels, preds, zero_division=0, average='binary')\n",
        "  f1 = f1_score(labels, preds, zero_division=0, average='binary')\n",
        "\n",
        "  return {\n",
        "    \"accuracy\": accuracy,\n",
        "    \"precision\": precision,\n",
        "    \"recall\": recall,\n",
        "    \"f1\": f1\n",
        "  }\n",
        "\n",
        "def find_optimal_threshold(trainer, val_dataset):\n",
        "  \"\"\"Find optimal threshold that maximizes F1 score on validation set, with constraints to avoid extreme predictions.\"\"\"\n",
        "  predictions = trainer.predict(val_dataset)\n",
        "  probs = torch.softmax(torch.tensor(predictions.predictions), dim=-1)\n",
        "  probs_class1 = probs[:, 1].numpy()\n",
        "  labels = predictions.label_ids\n",
        "\n",
        "  best_threshold = 0.5\n",
        "  best_f1 = 0\n",
        "  best_metrics = {}\n",
        "\n",
        "  # Get true class distribution\n",
        "  true_class1_ratio = np.mean(labels)\n",
        "\n",
        "  # Try different thresholds with wider search range to avoid extreme predictions\n",
        "  # Search more thoroughly across the probability range\n",
        "  for threshold in np.arange(0.1, 0.9, 0.01):\n",
        "    preds = (probs_class1 >= threshold).astype(int)\n",
        "\n",
        "    # Calculate metrics\n",
        "    f1 = f1_score(labels, preds, zero_division=0)\n",
        "    precision = precision_score(labels, preds, zero_division=0)\n",
        "    recall = recall_score(labels, preds, zero_division=0)\n",
        "\n",
        "    # Check if predictions are balanced (not all one class)\n",
        "    pred_class1_count = np.sum(preds)\n",
        "    total_samples = len(preds)\n",
        "    pred_class1_ratio = pred_class1_count / total_samples\n",
        "\n",
        "    # Skip thresholds that predict all as one class\n",
        "    if pred_class1_count == 0 or pred_class1_count == total_samples:\n",
        "      continue\n",
        "\n",
        "    # Prefer thresholds that give reasonable class balance\n",
        "    # Add small bonus for thresholds close to true distribution\n",
        "    balance_bonus = 1.0\n",
        "    if abs(pred_class1_ratio - true_class1_ratio) < 0.2:  # Within 20% of true ratio\n",
        "      balance_bonus = 1.05  # 5% bonus\n",
        "\n",
        "    # Score combines F1 with balance consideration\n",
        "    score = f1 * balance_bonus\n",
        "\n",
        "    if score > best_f1 or (score == best_f1 and abs(pred_class1_ratio - true_class1_ratio) < abs(best_metrics.get('pred_class1_ratio', 1) - true_class1_ratio)):\n",
        "      best_f1 = f1\n",
        "      best_threshold = threshold\n",
        "      best_metrics = {\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'pred_class1_ratio': pred_class1_ratio,\n",
        "        'true_class1_ratio': true_class1_ratio\n",
        "      }\n",
        "\n",
        "  # Fallback if no good threshold found\n",
        "  if best_threshold == 0.5 and best_f1 == 0:\n",
        "    print(\"Warning: No balanced threshold found in search, trying alternative approaches...\")\n",
        "\n",
        "    # Try using quantile-based threshold\n",
        "    threshold = np.percentile(probs_class1, (1 - true_class1_ratio) * 100)\n",
        "    preds = (probs_class1 >= threshold).astype(int)\n",
        "\n",
        "    if len(np.unique(preds)) > 1:  # If we get both classes\n",
        "      best_threshold = threshold\n",
        "      best_f1 = f1_score(labels, preds, zero_division=0)\n",
        "      best_metrics = {\n",
        "        'f1': best_f1,\n",
        "        'precision': precision_score(labels, preds, zero_division=0),\n",
        "        'recall': recall_score(labels, preds, zero_division=0),\n",
        "        'pred_class1_ratio': np.sum(preds) / len(preds),\n",
        "        'true_class1_ratio': true_class1_ratio\n",
        "      }\n",
        "    else:\n",
        "      # Last resort: use median\n",
        "      best_threshold = np.median(probs_class1)\n",
        "      preds = (probs_class1 >= best_threshold).astype(int)\n",
        "      best_f1 = f1_score(labels, preds, zero_division=0)\n",
        "      best_metrics = {\n",
        "        'f1': best_f1,\n",
        "        'precision': precision_score(labels, preds, zero_division=0),\n",
        "        'recall': recall_score(labels, preds, zero_division=0),\n",
        "        'pred_class1_ratio': np.sum(preds) / len(preds),\n",
        "        'true_class1_ratio': true_class1_ratio\n",
        "      }\n",
        "\n",
        "  print(f\"Optimal threshold: {best_threshold:.3f}\")\n",
        "  print(f\"  F1: {best_metrics.get('f1', 0):.4f}, Precision: {best_metrics.get('precision', 0):.4f}, Recall: {best_metrics.get('recall', 0):.4f}\")\n",
        "  print(f\"  Predicted class 1 ratio: {best_metrics.get('pred_class1_ratio', 0):.2%}\")\n",
        "  print(f\"  True class 1 ratio: {best_metrics.get('true_class1_ratio', 0):.2%}\")\n",
        "  return best_threshold\n",
        "\n",
        "def evaluate_model(trainer, test_dataset, threshold=None):\n",
        "  from sklearn.metrics import confusion_matrix  # Import here if not already imported\n",
        "  predictions = trainer.predict(test_dataset)\n",
        "  probs = torch.softmax(torch.tensor(predictions.predictions), dim=-1)\n",
        "  probs_class1 = probs[:, 1].numpy()\n",
        "  labels = predictions.label_ids\n",
        "\n",
        "  # Use optimal threshold if provided, otherwise use argmax (threshold=0.5)\n",
        "  if threshold is not None:\n",
        "    preds = (probs_class1 >= threshold).astype(int)\n",
        "    print(f\"Using threshold: {threshold:.3f}\")\n",
        "  else:\n",
        "    preds = np.argmax(predictions.predictions, axis=1)\n",
        "    print(\"Using default threshold: 0.5 (argmax)\")\n",
        "\n",
        "  # Show first 10 predictions as examples\n",
        "  print(\"Sample predictions (first 10):\")\n",
        "  for i, (label, pred) in enumerate(zip(labels[:10], preds[:10])):\n",
        "    print(f\"  True: {label}, Predicted: {pred}, Prob(class1): {probs_class1[i]:.3f}\")\n",
        "  if len(labels) > 10:\n",
        "    print(f\"  ... ({len(labels) - 10} more predictions)\")\n",
        "\n",
        "  accuracy = accuracy_score(labels, preds)\n",
        "  precision = precision_score(labels, preds, zero_division=0)\n",
        "  recall = recall_score(labels, preds, zero_division=0)\n",
        "  f1 = f1_score(labels, preds, zero_division=0)\n",
        "\n",
        "  # Calculate and print confusion matrix\n",
        "  cm = confusion_matrix(labels, preds)\n",
        "\n",
        "  print(f\"\\nConfusion Matrix:\")\n",
        "  print(f\"                Predicted\")\n",
        "  print(f\"              Non-Dep  Depressed\")\n",
        "  print(f\"Actual Non-Dep    {cm[0,0]:4d}      {cm[0,1]:4d}\")\n",
        "  print(f\"       Depressed   {cm[1,0]:4d}      {cm[1,1]:4d}\")\n",
        "  print(f\"\\nConfusion Matrix (detailed):\")\n",
        "  print(f\"  True Negatives (TN): {cm[0,0]} - Correctly predicted non-depressed\")\n",
        "  print(f\"  False Positives (FP): {cm[0,1]} - Non-depressed predicted as depressed\")\n",
        "  print(f\"  False Negatives (FN): {cm[1,0]} - Depressed predicted as non-depressed\")\n",
        "  print(f\"  True Positives (TP): {cm[1,1]} - Correctly predicted depressed\")\n",
        "\n",
        "  print(f\"\\nTest Metrics:\")\n",
        "  print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "  print(f\"  Precision: {precision:.4f}\")\n",
        "  print(f\"  Recall: {recall:.4f}\")\n",
        "  print(f\"  F1 Score: {f1:.4f}\")\n",
        "\n",
        "  return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"confusion_matrix\": cm}\n",
        "\n",
        "def train_model(df, save_model=True, model_save_path=\"./depression_classifier_model\"):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "  train_df = df[df['split'] == 'train'].reset_index(drop=True)\n",
        "  val_df = df[df['split'] == 'dev'].reset_index(drop=True)\n",
        "  test_df = df[df['split'] == 'test'].reset_index(drop=True)\n",
        "\n",
        "  print(f\"Training samples: {len(train_df)}\")\n",
        "  print(f\"Validation samples: {len(val_df)}\")\n",
        "  print(f\"Test samples: {len(test_df)}\")\n",
        "\n",
        "  # Calculate class weights to handle imbalanced dataset (for reference, though we use Focal Loss)\n",
        "  from sklearn.utils.class_weight import compute_class_weight\n",
        "  labels = train_df['depression_label'].values\n",
        "  classes = np.unique(labels)\n",
        "  class_weights_balanced = compute_class_weight('balanced', classes=classes, y=labels)\n",
        "\n",
        "  # Apply multiplier to strengthen minority class weight (step 4)\n",
        "  weight_multiplier = 1.8  # Increase weight for minority class\n",
        "  class_weights = class_weights_balanced.copy()\n",
        "  # Find minority class (class with fewer samples)\n",
        "  class_counts = train_df['depression_label'].value_counts().sort_index()\n",
        "  minority_class = class_counts.idxmin()\n",
        "  minority_class_idx = list(classes).index(minority_class)\n",
        "  class_weights[minority_class_idx] *= weight_multiplier\n",
        "\n",
        "  class_weights_dict = dict(zip(classes, class_weights))\n",
        "  print(f\"\\nClass distribution in training set:\")\n",
        "  print(train_df['depression_label'].value_counts().sort_index())\n",
        "  print(f\"Balanced class weights: {dict(zip(classes, class_weights_balanced))}\")\n",
        "  print(f\"Adjusted class weights (multiplier={weight_multiplier}x for minority): {class_weights_dict}\")\n",
        "  print(f\"Using Focal Loss (alpha=0.85, gamma=2.5) for better imbalance handling\")\n",
        "\n",
        "  train_dataset = TranscriptsDataset(train_df, tokenizer)\n",
        "  val_dataset = TranscriptsDataset(val_df, tokenizer)\n",
        "  test_dataset = TranscriptsDataset(test_df, tokenizer)\n",
        "\n",
        "  # Use Focal Loss with adjusted parameters for better learning and preventing all-one-class predictions\n",
        "  # Higher alpha (0.9) gives more weight to minority class, higher gamma (3.0) focuses more on hard examples\n",
        "  # These parameters help prevent the model from collapsing to predicting only one class\n",
        "  model = TextClassifier(MODEL_NAME, num_labels=2, use_focal_loss=True, focal_alpha=0.9, focal_gamma=3.0)\n",
        "\n",
        "  # Initialize classifier weights with small values to prevent extreme initial predictions\n",
        "  # This helps avoid the model starting with all-one-class predictions\n",
        "  with torch.no_grad():\n",
        "    nn.init.normal_(model.classifier.weight, mean=0.0, std=0.02)\n",
        "    nn.init.zeros_(model.classifier.bias)\n",
        "\n",
        "  print(\"\\nModel initialized with balanced weights to prevent extreme initial predictions\")\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",  # Evaluate every epoch\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10,  # Log more frequently to see training progress\n",
        "    learning_rate=1e-5,  # Slightly lower learning rate for more stable training\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    num_train_epochs=10,  # More epochs to allow better learning\n",
        "    gradient_accumulation_steps=4,\n",
        "    fp16=True,\n",
        "    save_strategy=\"epoch\",  # Save every epoch\n",
        "    load_best_model_at_end=True,  # Load the best model at the end\n",
        "    metric_for_best_model=\"eval_f1\",  # Use F1 score instead of eval_loss as the primary metric\n",
        "    greater_is_better=True,  # F1 score should be maximized (higher is better)\n",
        "    warmup_steps=10,  # Add warmup for better training stability\n",
        "    weight_decay=0.01,  # Add weight decay for regularization\n",
        "    save_total_limit=3,  # Keep only the best 3 models based on F1 score\n",
        "    report_to=\"none\",  # Disable wandb/tensorboard to reduce overhead\n",
        "  )\n",
        "\n",
        "  # Create early stopping callback - it will automatically use eval_f1 from TrainingArguments\n",
        "  # The callback will use the metric specified in metric_for_best_model\n",
        "  early_stopping = EarlyStoppingCallback(\n",
        "    early_stopping_patience=3,  # Stop if F1 doesn't improve for 3 epochs\n",
        "    early_stopping_threshold=0.001  # Minimum improvement threshold\n",
        "  )\n",
        "\n",
        "  # training_args = TrainingArguments(\n",
        "  #   output_dir=\"./results\",\n",
        "\n",
        "  #   # --- LOGGING ---\n",
        "  #   logging_strategy=\"steps\",\n",
        "  #   logging_steps=1,                 # Log every step\n",
        "  #   report_to=\"none\",                # or \"tensorboard\"\n",
        "  #   log_level=\"info\",\n",
        "  #   log_level_replica=\"info\",\n",
        "\n",
        "  #   # --- EVAL DURING TRAINING ---\n",
        "  #   eval_strategy=\"steps\",     # Run evaluation more frequently\n",
        "  #   eval_steps=10,                   # Eval every 10 steps\n",
        "  #   save_strategy=\"steps\",           # Save more often\n",
        "  #   save_steps=10,\n",
        "\n",
        "  #   # --- TRAINING HYPERPARAMETERS ---\n",
        "  #   learning_rate=1e-5,\n",
        "  #   per_device_train_batch_size=1,\n",
        "  #   per_device_eval_batch_size=1,\n",
        "  #   gradient_accumulation_steps=4,\n",
        "  #   num_train_epochs=1,\n",
        "  #   warmup_steps=10,\n",
        "  #   weight_decay=0.01,\n",
        "  #   fp16=True,\n",
        "\n",
        "  #   # --- BEST MODEL ---\n",
        "  #   load_best_model_at_end=True,\n",
        "  #   metric_for_best_model=\"eval_f1\",\n",
        "  #   greater_is_better=True,\n",
        "\n",
        "  #   # --- MISC ---\n",
        "  #   save_total_limit=3,    # Keep only last 3 checkpoints\n",
        "  # )\n",
        "\n",
        "  trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,  # Compute F1, precision, recall, accuracy\n",
        "    callbacks=[early_stopping],  # Add early stopping based on F1 score\n",
        "  )\n",
        "\n",
        "  print(\"Training configuration:\")\n",
        "  print(f\"  Primary metric: eval_f1 (F1 score)\")\n",
        "  print(f\"  Model selection: Best model selected based on highest F1 score\")\n",
        "  print(f\"  Early stopping: Stops if F1 doesn't improve for 3 epochs\")\n",
        "  print(f\"  Loss function: Focal Loss (for training, but model selection uses F1)\")\n",
        "\n",
        "  print(\"\\nStarting training...\")\n",
        "  trainer.train()\n",
        "\n",
        "  # Find optimal threshold on validation set (step 3)\n",
        "  print(\"\\nFinding optimal threshold on validation set...\")\n",
        "\n",
        "  # First, check probability distribution on validation set\n",
        "  val_predictions = trainer.predict(val_dataset)\n",
        "  val_probs = torch.softmax(torch.tensor(val_predictions.predictions), dim=-1)\n",
        "  val_probs_class1 = val_probs[:, 1].numpy()\n",
        "  print(f\"\\nValidation set probability distribution (class 1):\")\n",
        "  print(f\"  Min: {val_probs_class1.min():.4f}, Max: {val_probs_class1.max():.4f}\")\n",
        "  print(f\"  Mean: {val_probs_class1.mean():.4f}, Std: {val_probs_class1.std():.4f}\")\n",
        "  print(f\"  Median: {np.median(val_probs_class1):.4f}\")\n",
        "\n",
        "  optimal_threshold = find_optimal_threshold(trainer, val_dataset)\n",
        "\n",
        "  print(\"\\nEvaluating on test set...\")\n",
        "  metrics = evaluate_model(trainer, test_dataset, threshold=optimal_threshold)\n",
        "\n",
        "  if save_model:\n",
        "    print(f\"\\nSaving model to {model_save_path}...\")\n",
        "    trainer.save_model(model_save_path)\n",
        "    tokenizer.save_pretrained(model_save_path)\n",
        "    print(\"Model saved successfully!\")\n",
        "\n",
        "  return trainer, metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XVQ6bjz5wMdH",
      "metadata": {
        "id": "XVQ6bjz5wMdH"
      },
      "source": [
        "## Load and Explore Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cTaYSHEUwS0H",
      "metadata": {
        "id": "cTaYSHEUwS0H"
      },
      "source": [
        "## Generate topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "8xt-n1q0wVUy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xt-n1q0wVUy",
        "outputId": "259012d3-d7d0-4798-ecaa-18da148fa24b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing training transcripts...\n",
            "Processing validation transcripts...\n",
            "Processing test transcripts...\n",
            "\n",
            "✅ Topic transcripts generated successfully!\n",
            "   Files saved: 219 transcript files\n",
            "   Total rows: 5615\n",
            "   Training: 2954 rows\n",
            "   Validation: 1292 rows\n",
            "   Test: 1369 rows\n",
            "\n",
            "   Output directory: /content/daic_data/topic_transcripts/transcripts\n"
          ]
        }
      ],
      "source": [
        "def train_index(train_dir, val_dir, test_dir):\n",
        "  train_df = pd.read_csv(train_dir)\n",
        "  val_df = pd.read_csv(val_dir)\n",
        "  test_df = pd.read_csv(test_dir)\n",
        "\n",
        "  train_df = train_df[[\"Participant_ID\", \"Gender\"]].copy()\n",
        "  test_df = test_df[[\"Participant_ID\", \"Gender\"]].copy()\n",
        "  val_df = val_df[[\"Participant_ID\", \"Gender\"]].copy()\n",
        "\n",
        "  return train_df, val_df, test_df\n",
        "\n",
        "\n",
        "def topic_selection(transcript):\n",
        "  interest = [\n",
        "    \"recently that you really enjoy\",\n",
        "    \"traveling\",\n",
        "    \"travel alot\",\n",
        "    \"family\",\n",
        "    \"fun\",\n",
        "    \"best friend\",\n",
        "    \"weekend\",\n",
        "  ]\n",
        "  sleep = [\n",
        "    \"good night's sleep\",\n",
        "    \"don't sleep well\"\n",
        "  ]\n",
        "  feeling_depressed = [\n",
        "    \"really happy\",\n",
        "    \"behavior\",\n",
        "    \" disturbing thought\",\n",
        "    \"feel_lately\",\n",
        "  ]\n",
        "  failure = [\n",
        "    \"regret\",\n",
        "    \"guilty\",\n",
        "    \"proud\",\n",
        "    \"being_parent\",\n",
        "    \"best_quality\"\n",
        "  ]\n",
        "  personality = [\n",
        "    \"introvert\",\n",
        "    \"shyoutgoing\"\n",
        "  ]\n",
        "  dignose = [\n",
        "    \"ptsd\",\n",
        "    \"depression\",\n",
        "    \"therapy is useful\"\n",
        "  ]\n",
        "  parent = [\n",
        "    \"hard_parent\",\n",
        "    \"best_parent\",\n",
        "    \"easy_parent\",\n",
        "    \"your_kid\",\n",
        "    \"differnet_parent\",\n",
        "  ]\n",
        "\n",
        "  ques = [interest, sleep, feeling_depressed, failure, personality, dignose, parent]\n",
        "  topic_name = []\n",
        "  for topic_count, topic in enumerate(ques):\n",
        "    for sub_topic_count, sub_topic in enumerate(topic):\n",
        "      # remove nan\n",
        "      if type(transcript) == float:\n",
        "        print(transcript)\n",
        "        return \"problem\"\n",
        "      if type(transcript) != float:\n",
        "        if sub_topic in transcript:\n",
        "          topic_name.append([topic_count, sub_topic_count])\n",
        "\n",
        "  return topic_name\n",
        "\n",
        "\n",
        "def data_retrieve(working_dir, train_id, should_save_no_topic=False):\n",
        "  participants = train_id\n",
        "  transcripts = pd.DataFrame()\n",
        "\n",
        "  for index_p, row in participants.iterrows():\n",
        "    filename = str(int(row.Participant_ID)) + \"_TRANSCRIPT.csv\"\n",
        "\n",
        "    location = os.path.join(working_dir, filename)\n",
        "    if not os.path.exists(location):\n",
        "      continue\n",
        "\n",
        "    temp = pd.read_csv(location, sep=\"\\t\")\n",
        "    temp = temp.dropna(subset=[\"value\"])\n",
        "    temp = temp.reset_index(drop=True)  # Reset index to ensure sequential 0-based indexing\n",
        "    temp[\"topic\"] = pd.Series(dtype=\"object\")\n",
        "    temp[\"topic_value\"] = pd.Series(dtype=\"object\")\n",
        "    temp[\"sub_topic\"] = pd.Series(dtype=\"object\")\n",
        "    temp[\"participant\"] = row.Participant_ID\n",
        "\n",
        "    found_any_topic = False\n",
        "    max_index = len(temp) - 1  # Get maximum valid index\n",
        "\n",
        "    for index_t, row_t in temp.iterrows():\n",
        "      if row_t.speaker == \"Ellie\":\n",
        "        topic = topic_selection(row_t.value)\n",
        "\n",
        "        if topic != [] and len(topic) > 1:\n",
        "          df_try = row_t\n",
        "          temp.append([df_try] * len(topic), ignore_index=True)\n",
        "\n",
        "        for words in topic:\n",
        "          found_any_topic = True\n",
        "          check = False\n",
        "\n",
        "          # Check bounds before accessing index_t + 1\n",
        "          if index_t + 1 <= max_index and temp.iloc[index_t + 1][\"speaker\"] == \"Participant\":\n",
        "            temp.loc[index_t + 1, \"topic\"] = words[0]\n",
        "            temp.loc[index_t + 1, \"sub_topic\"] = words[1]\n",
        "            temp.loc[index_t + 1, \"topic_value\"] = row_t.value\n",
        "\n",
        "          # Check bounds before accessing index_t + 2\n",
        "          if index_t + 2 <= max_index and temp.iloc[index_t + 2][\"speaker\"] == \"Participant\":\n",
        "            check = True\n",
        "            temp.loc[index_t + 2, \"topic\"] = words[0]\n",
        "            temp.loc[index_t + 2, \"sub_topic\"] = words[1]\n",
        "            temp.loc[index_t + 2, \"topic_value\"] = row_t.value\n",
        "\n",
        "          # Check bounds before accessing index_t + 3\n",
        "          if index_t + 3 <= max_index and check and temp.iloc[index_t + 3][\"speaker\"] == \"Participant\":\n",
        "            temp.loc[index_t + 3, \"topic\"] = words[0]\n",
        "            temp.loc[index_t + 3, \"sub_topic\"] = words[1]\n",
        "            temp.loc[index_t + 3, \"topic_value\"] = row_t.value\n",
        "\n",
        "    if not found_any_topic and should_save_no_topic:\n",
        "      temp[\"topic\"] = -1\n",
        "      temp[\"sub_topic\"] = -1\n",
        "      temp[\"topic_value\"] = \"No_topic_found\"\n",
        "\n",
        "    temp.dropna(inplace=True)\n",
        "    transcripts = pd.concat([transcripts, temp], axis=0)\n",
        "\n",
        "  return transcripts\n",
        "\n",
        "\n",
        "def generate_topic_transcripts(train_file, val_file, test_file, transcritps_dir, out_dir):\n",
        "  \"\"\"\n",
        "  Generate topic-labeled transcripts and save them in the same structure as daic_data.\n",
        "  Creates individual transcript files per participant in out_dir/transcripts/ folder.\n",
        "  \"\"\"\n",
        "  # Create output directory structure matching daic_data\n",
        "  out_transcripts_dir = os.path.join(out_dir, \"transcripts\")\n",
        "  os.makedirs(out_transcripts_dir, exist_ok=True)\n",
        "\n",
        "  train_df, val_df, test_df = train_index(train_file, val_file, test_file)\n",
        "\n",
        "  print(\"Processing training transcripts...\")\n",
        "  train_transcripts = data_retrieve(transcritps_dir, train_df)\n",
        "\n",
        "  print(\"Processing validation transcripts...\")\n",
        "  val_transcripts = data_retrieve(transcritps_dir, val_df, should_save_no_topic=True)\n",
        "\n",
        "  print(\"Processing test transcripts...\")\n",
        "  test_transcripts = data_retrieve(transcritps_dir, test_df, should_save_no_topic=True)\n",
        "\n",
        "  # Combine all transcripts and group by participant\n",
        "  # This ensures we handle each participant only once, even if they appear in multiple splits\n",
        "  all_transcripts_combined = pd.concat([train_transcripts, val_transcripts, test_transcripts], axis=0, ignore_index=True)\n",
        "\n",
        "  files_saved = 0\n",
        "  total_rows = 0\n",
        "\n",
        "  # Group by participant and save individual files (matching daic_data structure)\n",
        "  if len(all_transcripts_combined) > 0:\n",
        "    for participant_id in all_transcripts_combined['participant'].unique():\n",
        "      participant_data = all_transcripts_combined[all_transcripts_combined['participant'] == participant_id].copy()\n",
        "\n",
        "      # Save transcript file with same naming convention as original\n",
        "      filename = f\"{int(participant_id)}_TRANSCRIPT.csv\"\n",
        "      file_path = os.path.join(out_transcripts_dir, filename)\n",
        "\n",
        "      # Select and order columns to match original structure, adding topic columns\n",
        "      columns_to_save = ['speaker', 'start_time', 'stop_time', 'value']\n",
        "      if 'topic' in participant_data.columns:\n",
        "        columns_to_save.extend(['topic', 'sub_topic', 'topic_value'])\n",
        "      if 'participant' in participant_data.columns:\n",
        "        columns_to_save.append('participant')\n",
        "\n",
        "      # Only save columns that exist in the dataframe\n",
        "      available_columns = [col for col in columns_to_save if col in participant_data.columns]\n",
        "      participant_data = participant_data[available_columns]\n",
        "\n",
        "      # Sort by start_time to maintain chronological order\n",
        "      if 'start_time' in participant_data.columns:\n",
        "        participant_data = participant_data.sort_values(by='start_time').reset_index(drop=True)\n",
        "\n",
        "      participant_data.to_csv(file_path, index=False, sep=\"\\t\", encoding=\"utf-8\")\n",
        "      files_saved += 1\n",
        "      total_rows += len(participant_data)\n",
        "\n",
        "  print(f\"\\n✅ Topic transcripts generated successfully!\")\n",
        "  print(f\"   Files saved: {files_saved} transcript files\")\n",
        "  print(f\"   Total rows: {total_rows}\")\n",
        "  print(f\"   Training: {len(train_transcripts)} rows\")\n",
        "  print(f\"   Validation: {len(val_transcripts)} rows\")\n",
        "  print(f\"   Test: {len(test_transcripts)} rows\")\n",
        "  print(f\"\\n   Output directory: {out_transcripts_dir}\")\n",
        "\n",
        "\n",
        "# Generate topic transcripts\n",
        "train_file = os.path.join(DATA_DIR, \"labels\", \"train.csv\")\n",
        "val_file = os.path.join(DATA_DIR, \"labels\", \"dev.csv\")\n",
        "test_file = os.path.join(DATA_DIR, \"labels\", \"test.csv\")\n",
        "transcripts_dir = os.path.join(DATA_DIR, \"transcripts\")\n",
        "out_dir = os.path.join(DATA_DIR, \"topic_transcripts\")\n",
        "\n",
        "generate_topic_transcripts(train_file, val_file, test_file, transcripts_dir, out_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "pVjMh8SUziUU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "id": "pVjMh8SUziUU",
        "outputId": "7b52000e-6ab2-422d-fbb0-9ac43a3ed0f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 219\n",
            "\n",
            "Split distribution:\n",
            "split\n",
            "train    137\n",
            "test      47\n",
            "dev       35\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Label distribution:\n",
            "depression_label\n",
            "0    133\n",
            "1     86\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Label distribution by split:\n",
            "split  depression_label\n",
            "dev    0                   23\n",
            "       1                   12\n",
            "test   0                   33\n",
            "       1                   14\n",
            "train  0                   77\n",
            "       1                   60\n",
            "dtype: int64\n",
            "\n",
            "First few rows:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  depression_label split\n",
              "0  i'm very close sometimes too close no mm not o...                 0   dev\n",
              "1  um uh going out with friends going to bars goi...                 0   dev\n",
              "2  no no i'm much <mu> much of more <m> more of a...                 0   dev\n",
              "3  no i don't  um  the last time i felt really ha...                 1   dev\n",
              "4  i just like um going to new places and just di...                 1   dev"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3b0d0b42-9629-4d98-8013-b8662f388b87\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>depression_label</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i'm very close sometimes too close no mm not o...</td>\n",
              "      <td>0</td>\n",
              "      <td>dev</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>um uh going out with friends going to bars goi...</td>\n",
              "      <td>0</td>\n",
              "      <td>dev</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>no no i'm much &lt;mu&gt; much of more &lt;m&gt; more of a...</td>\n",
              "      <td>0</td>\n",
              "      <td>dev</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>no i don't  um  the last time i felt really ha...</td>\n",
              "      <td>1</td>\n",
              "      <td>dev</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i just like um going to new places and just di...</td>\n",
              "      <td>1</td>\n",
              "      <td>dev</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3b0d0b42-9629-4d98-8013-b8662f388b87')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3b0d0b42-9629-4d98-8013-b8662f388b87 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3b0d0b42-9629-4d98-8013-b8662f388b87');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6af466e2-daf7-4255-bf79-149554d0607d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6af466e2-daf7-4255-bf79-149554d0607d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6af466e2-daf7-4255-bf79-149554d0607d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 219,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 219,\n        \"samples\": [\n          \"### Response: i don't know if I'd say that but yes \\u25b6\\u25b6<> ### Response: uh we have a very good relationship with each other so it's always just been easy for us to be together um \\u25b6 ### Response: um she said 'i'm a kind person' getInstance ### Response: i am but not currently ienn<laughter> ### Response: um i don't know \\u0219te i don't know  quanto  gostar\\u00edamos de saber  quanto  quanto se \\u00e9 \\u00fatil o que queremos verificar  q est\\u00e1 <laughter> ienn ### Response: um i haven't really changed much in terms of like my behavior i guess i just feel like i want to be here for longer periods of time than just a few days um but that doesn't mean that i don't want to leave you know glise  bay<laughter>  quantoe  bay ### Response: um it depends on if i'm tired it depends on if i'm too busy um but yeah it can be hard at times iginator_ ### Response: i feel like i'm not really myself anymore  Bev\\u00f6lkerzung<\\u300b ### Response: uh probably last week um it wasn't a big thing but yeah \\u25b6zeugung <conversation> \\u25b6 ### Response: um i don't really regret anything i don't really regret anything in my life i guess except for maybe not getting married i guess but i don't regret that either  yes ienn't really regretted anything in my life i've been pretty lucky  <laughter>  cc ### Response: um well i guess uh my children have been the most rewarding thing for me \\u25b6\\u25b6\",\n          \"mm depends on the situation or who i'm around when i first meet people i can be shy uh but if it's someone i've known for a while i can be outgoing um i guess i feel guilty about the fact that i don't talk to my grandparents as much as i should um i'm close with my grandparents but not really anyone else in my family uh not really super easy um i sort of i mean it's my probably my fault i go to bed too late just sort of foggy and distracted i have trouble concentrating you mean like my body no it's been the same as ever no no uh last night i was just hanging out with my friends and was just having like a really good time sharing stories and laughing and stuff uh probably funny and uh easy to get along with but also caring uh i spend my ideal weekend not really doing much like maybe just sitting around and reading or going to the movies watching t_v uh no i don't think um i guess i'm most proud of the fact that uh i\",\n          \"sometimes i'm shy but i'm really an outgoing person and a people person um probably not working right now i need a job <laughter> um my relationship with my family i don't have i just really stay with my dad and i have a daughter sometimes it's easy but uh sometimes it gets a little overwhelming oh it's very easy i sleep well every night i can sleep any time i sleep very well um a little sad but most of the time i'm very happy i'm just not happy with where i am in my life right now um not really i really i try to think very positive and try to do whatever i can every day to look for work and just stay positive and focused no i haven't no i haven't um the last time i felt very happy was when i graduated from um college i felt very happy that was one of the best things that happened in my life and that was the last time i was really really very happy like that um right now i don't have <ha> really have a best friend i really try to i'm yeah just being friends with certain people that don't have positive things going on with themselves or just don't it just when i notice things about people that should determine whether i wanna be their friend or not you know they might have well i went um to a friend's house that i haven't seen since childhood and they um i'm proud that i did get my g_e_d i did graduate\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"depression_label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"split\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"dev\",\n          \"train\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "# Load the data\n",
        "df = process_daic_data(DATA_DIR)\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(f\"Total samples: {len(df)}\")\n",
        "print(f\"\\nSplit distribution:\")\n",
        "print(df['split'].value_counts())\n",
        "print(f\"\\nLabel distribution:\")\n",
        "print(df['depression_label'].value_counts())\n",
        "print(f\"\\nLabel distribution by split:\")\n",
        "print(df.groupby(['split', 'depression_label']).size())\n",
        "\n",
        "# Display first few rows\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MFIEwHBRwow_",
      "metadata": {
        "id": "MFIEwHBRwow_"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "0GiMcfirwl-u",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0GiMcfirwl-u",
        "outputId": "bf6a9a7c-e678-4844-f76a-197df913cfb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 137\n",
            "Validation samples: 35\n",
            "Test samples: 47\n",
            "\n",
            "Class distribution in training set:\n",
            "depression_label\n",
            "0    77\n",
            "1    60\n",
            "Name: count, dtype: int64\n",
            "Balanced class weights: {np.int64(0): np.float64(0.8896103896103896), np.int64(1): np.float64(1.1416666666666666)}\n",
            "Adjusted class weights (multiplier=1.8x for minority): {np.int64(0): np.float64(0.8896103896103896), np.int64(1): np.float64(2.055)}\n",
            "Using Focal Loss (alpha=0.85, gamma=2.5) for better imbalance handling\n",
            "\n",
            "Model initialized with balanced weights to prevent extreme initial predictions\n",
            "Training configuration:\n",
            "  Primary metric: eval_f1 (F1 score)\n",
            "  Model selection: Best model selected based on highest F1 score\n",
            "  Early stopping: Stops if F1 doesn't improve for 3 epochs\n",
            "  Loss function: Focal Loss (for training, but model selection uses F1)\n",
            "\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='140' max='350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [140/350 06:32 < 09:57, 0.35 it/s, Epoch 4/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.079100</td>\n",
              "      <td>0.080819</td>\n",
              "      <td>0.657143</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.080300</td>\n",
              "      <td>0.079599</td>\n",
              "      <td>0.657143</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.079300</td>\n",
              "      <td>0.078466</td>\n",
              "      <td>0.657143</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.081600</td>\n",
              "      <td>0.077909</td>\n",
              "      <td>0.657143</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Finding optimal threshold on validation set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation set probability distribution (class 1):\n",
            "  Min: 0.5100, Max: 0.5160\n",
            "  Mean: 0.5123, Std: 0.0013\n",
            "  Median: 0.5125\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No balanced threshold found in search, trying alternative approaches...\n",
            "Optimal threshold: 0.513\n",
            "  F1: 0.5000, Precision: 0.5000, Recall: 0.5000\n",
            "  Predicted class 1 ratio: 34.29%\n",
            "  True class 1 ratio: 34.29%\n",
            "\n",
            "Evaluating on test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using threshold: 0.513\n",
            "Sample predictions (first 10):\n",
            "  True: 0, Predicted: 0, Prob(class1): 0.512\n",
            "  True: 0, Predicted: 1, Prob(class1): 0.513\n",
            "  True: 0, Predicted: 0, Prob(class1): 0.512\n",
            "  True: 1, Predicted: 1, Prob(class1): 0.514\n",
            "  True: 1, Predicted: 0, Prob(class1): 0.513\n",
            "  True: 1, Predicted: 0, Prob(class1): 0.513\n",
            "  True: 0, Predicted: 1, Prob(class1): 0.514\n",
            "  True: 0, Predicted: 0, Prob(class1): 0.512\n",
            "  True: 0, Predicted: 0, Prob(class1): 0.512\n",
            "  True: 1, Predicted: 0, Prob(class1): 0.512\n",
            "  ... (37 more predictions)\n",
            "\n",
            "Confusion Matrix:\n",
            "                Predicted\n",
            "              Non-Dep  Depressed\n",
            "Actual Non-Dep      29         4\n",
            "       Depressed     10         4\n",
            "\n",
            "Confusion Matrix (detailed):\n",
            "  True Negatives (TN): 29 - Correctly predicted non-depressed\n",
            "  False Positives (FP): 4 - Non-depressed predicted as depressed\n",
            "  False Negatives (FN): 10 - Depressed predicted as non-depressed\n",
            "  True Positives (TP): 4 - Correctly predicted depressed\n",
            "\n",
            "Test Metrics:\n",
            "  Accuracy: 0.7021\n",
            "  Precision: 0.5000\n",
            "  Recall: 0.2857\n",
            "  F1 Score: 0.3636\n",
            "\n",
            "Saving model to ./depression_classifier_model...\n",
            "Model saved successfully!\n"
          ]
        }
      ],
      "source": [
        "trainer, metrics = train_model(df, save_model=True, model_save_path=\"./depression_classifier_model\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}