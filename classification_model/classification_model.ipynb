{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e5ab3af",
   "metadata": {},
   "source": [
    "# Classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765851fb",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81d15e0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd1cfe8",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2e6dba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def process_daic_data(data_dir):\n",
    "  transcripts_dir = os.path.join(data_dir, \"transcripts\")\n",
    "  labels_dir = os.path.join(data_dir, \"labels\")\n",
    "\n",
    "  df = pd.DataFrame()\n",
    "\n",
    "  for file in os.listdir(labels_dir):\n",
    "    if not file.endswith(\".csv\"):\n",
    "      continue\n",
    "\n",
    "    split_name = file.replace(\".csv\", \"\")\n",
    "    split_df = pd.read_csv(os.path.join(labels_dir, file))\n",
    "    split_df = split_df.rename(columns={\n",
    "      \"PHQ_Binary\": \"depression_label\",\n",
    "      \"PHQ_Score\": \"depression_severity\",\n",
    "      \"PHQ8_Binary\": \"depression_label\",\n",
    "      \"PHQ8_Score\": \"depression_severity\",\n",
    "      \"Participant_ID\": \"participant_id\",\n",
    "    })\n",
    "\n",
    "    transcripts_df = create_dataframe(split_df, transcripts_dir)\n",
    "    transcripts_df[\"split\"] = split_name\n",
    "\n",
    "    df = pd.concat([df, transcripts_df], ignore_index=True)\n",
    "\n",
    "  return df\n",
    "\n",
    "def create_dataframe(split_df, transcripts_dir):\n",
    "  df = {\"text\": [], \"depression_label\": []}\n",
    "\n",
    "  for _, row in split_df.iterrows():\n",
    "    participant_id = str(int(float(row.participant_id)))\n",
    "    depression_label = int(row.depression_label)\n",
    "\n",
    "    participant_text = \"\"\n",
    "    transcript_file = os.path.join(transcripts_dir, f\"{participant_id}_TRANSCRIPT.csv\")\n",
    "    if not os.path.exists(transcript_file):\n",
    "      print(f\"Transcript file not found for participant {participant_id}\")\n",
    "      continue\n",
    "\n",
    "    transcripts = pd.read_csv(transcript_file, sep=\"\\t\")\n",
    "    participant_transcripts = transcripts[transcripts['speaker'] == 'Participant']\n",
    "\n",
    "    for _, transcript_row in participant_transcripts.iterrows():\n",
    "      participant_text += str(transcript_row.value) + \" \"\n",
    "\n",
    "    df[\"text\"].append(participant_text.strip())\n",
    "    df[\"depression_label\"].append(depression_label)\n",
    "\n",
    "  return pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff62930",
   "metadata": {},
   "source": [
    "## Train classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1df307c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "MODEL_NAME = \"allenai/longformer-base-4096\"\n",
    "\n",
    "class TranscriptsDataset(Dataset):\n",
    "  def __init__(self, dataframe, tokenizer, max_length=4096):\n",
    "    self.data = dataframe\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_length = max_length\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    text = str(self.data.iloc[idx][\"text\"])\n",
    "    label = int(self.data.iloc[idx][\"depression_label\"])\n",
    "\n",
    "    encoding = self.tokenizer(\n",
    "      text,\n",
    "      truncation=True,\n",
    "      padding=\"max_length\",\n",
    "      max_length=self.max_length,\n",
    "      return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "      \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "      \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "    }\n",
    "  \n",
    "class TextFeaturizer(nn.Module):\n",
    "  def __init__(self, model_name, dropout=0.5, dense_size=256,\n",
    "               lora_r=8, lora_alpha=16, lora_dropout=0.1):\n",
    "    super().__init__() \n",
    "\n",
    "    # Load Longformer encoder \n",
    "    self.encoder = AutoModel.from_pretrained(model_name)\n",
    "    hidden_size = self.encoder.config.hidden_size\n",
    "\n",
    "    self.projection = nn.Sequential(\n",
    "      nn.Linear(hidden_size, dense_size),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(dropout)\n",
    "    )\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "      r=lora_r,\n",
    "      lora_alpha=lora_alpha,\n",
    "      target_modules=[\"query\", \"key\", \"value\"],\n",
    "      lora_dropout=lora_dropout,\n",
    "      bias=\"none\",\n",
    "      task_type=\"FEATURE_EXTRACTION\"\n",
    "    )\n",
    "    self.encoder = get_peft_model(self.encoder, lora_config)\n",
    "\n",
    "    for name, param in self.encoder.named_parameters():\n",
    "      if 'lora' not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    cls_token = outputs.last_hidden_state[:, 0]\n",
    "    return self.projection(cls_token)\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "  def __init__(self, model_name, num_labels=2):\n",
    "    super().__init__()\n",
    "    self.featurizer = TextFeaturizer(model_name)\n",
    "    self.classifier = nn.Linear(256, num_labels)\n",
    "\n",
    "  def forward(self, input_ids, attention_mask, labels=None):\n",
    "    features = self.featurizer(input_ids, attention_mask)\n",
    "    logits = self.classifier(features)\n",
    "\n",
    "    if labels is not None:\n",
    "      loss_fn = nn.CrossEntropyLoss()\n",
    "      loss = loss_fn(logits, labels)\n",
    "      return {\"loss\": loss, \"logits\": logits}\n",
    "    return {\"logits\": logits}\n",
    "\n",
    "def evaluate_model(trainer, test_dataset):\n",
    "  predictions = trainer.predict(test_dataset)\n",
    "  preds = np.argmax(predictions.predictions, axis=1)\n",
    "  labels = predictions.label_ids\n",
    "\n",
    "  for label, pred in zip(labels, preds):\n",
    "    print(f\"True: {label}, Predicted: {pred}\")\n",
    "\n",
    "  accuracy = accuracy_score(labels, preds)\n",
    "  precision = precision_score(labels, preds)\n",
    "  recall = recall_score(labels, preds)\n",
    "  f1 = f1_score(labels, preds)\n",
    "\n",
    "  print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "  print(f\"Test Precision: {precision:.4f}\")\n",
    "  print(f\"Test Recall: {recall:.4f}\")\n",
    "  print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n",
    "def train_model(df):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "  train_df = df[df['split'] == 'train'].reset_index(drop=True)\n",
    "  val_df = df[df['split'] == 'validation'].reset_index(drop=True)\n",
    "  test_df = df[df['split'] == 'test'].reset_index(drop=True)\n",
    "\n",
    "  train_dataset = TranscriptsDataset(train_df, tokenizer)\n",
    "  val_dataset = TranscriptsDataset(val_df, tokenizer)\n",
    "  test_dataset = TranscriptsDataset(test_df, tokenizer)\n",
    "  \n",
    "  model = TextClassifier(MODEL_NAME, num_labels=2)\n",
    "\n",
    "  training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=10,\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=True,\n",
    "  )\n",
    "\n",
    "  trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "  )\n",
    "  trainer.train()\n",
    "\n",
    "  evaluate_model(trainer, test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca574107",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
