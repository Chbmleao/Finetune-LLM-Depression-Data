{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMaAhfOrazZp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import glob, os\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5bd2cd5"
      },
      "outputs": [],
      "source": [
        "!unzip /content/daic_data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2AGaCQexbCS"
      },
      "source": [
        "## Data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Php58L8LbQNv"
      },
      "outputs": [],
      "source": [
        "def get_questions_answers_df(transcripts_dir):\n",
        "  transcripts_files = glob.glob(os.path.join(transcripts_dir, \"*.csv\"))\n",
        "\n",
        "  # Load and concatenate all transcript files\n",
        "  df = pd.concat(\n",
        "    (\n",
        "      pd.read_csv(file, sep=\"\\t\", encoding=\"utf-8-sig\").assign(source=os.path.basename(file))\n",
        "      for file in transcripts_files\n",
        "    ),\n",
        "    ignore_index=True\n",
        "  )\n",
        "\n",
        "  # Create block_id to identify contiguous speaker segments\n",
        "  df['block_id'] = (df['speaker'] != df['speaker'].shift(1)).cumsum()\n",
        "\n",
        "  # Aggregate by source and block_id to merge contiguous segments by the same speaker\n",
        "  df = df.groupby(['source', 'block_id']).agg(\n",
        "    speaker=('speaker', 'first'),\n",
        "    start_time=('start_time', 'min'),\n",
        "    stop_time=('stop_time', 'max'),\n",
        "    value=('value', lambda x: ' '.join(x.astype(str)))\n",
        "  )\n",
        "\n",
        "  # Sort by participant and time\n",
        "  df = df.sort_values(by=['source', 'start_time']).reset_index()\n",
        "\n",
        "  # Add previous speaker and value columns only if the previous source is the same\n",
        "  df['prev_speaker'] = df.groupby('source')['speaker'].shift(1)\n",
        "  df['prev_value'] = df.groupby('source')['value'].shift(1)\n",
        "\n",
        "  is_answer = (\n",
        "    (df['speaker'] == 'Participant') &\n",
        "    (df['prev_speaker'] == 'Ellie') &\n",
        "    (df['source'] == df['source'].shift(1))\n",
        "  )\n",
        "\n",
        "  df = df[is_answer].copy()\n",
        "  df = df.rename(columns={\n",
        "    'prev_value': 'question', # The previous Ellie utterance is the question\n",
        "    'value': 'answer',            # The current Participant utterance is the answer\n",
        "  })\n",
        "\n",
        "  df['participant_id'] = df['source'].str.split(\"_\").str[0].astype(int)\n",
        "  df = df[['participant_id', 'question', 'answer', 'start_time']]\n",
        "\n",
        "  return df\n",
        "\n",
        "def add_labels_to_df(qa_df, labels_dir):\n",
        "  splits = ['train', 'dev', 'test']\n",
        "\n",
        "  all_labels_df = pd.DataFrame()\n",
        "  for split in splits:\n",
        "    split_labels_df = pd.read_csv(os.path.join(labels_dir, f\"{split}.csv\"))\n",
        "    split_labels_df = split_labels_df.rename(columns={\n",
        "      \"Participant_ID\": \"participant_id\",\n",
        "      \"PHQ8_Binary\": \"depression_label\",\n",
        "      \"PHQ8_Score\": \"depression_severity\",\n",
        "      \"PHQ_Binary\": \"depression_label\",\n",
        "      \"PHQ_Score\": \"depression_severity\",\n",
        "    })\n",
        "    split_labels_df = split_labels_df[[\"participant_id\", \"depression_label\", \"depression_severity\"]]\n",
        "    split_labels_df[\"split\"] = split\n",
        "    all_labels_df = pd.concat([all_labels_df, split_labels_df], ignore_index=True)\n",
        "\n",
        "  merged_df = pd.merge(qa_df, all_labels_df, on=\"participant_id\", how=\"left\")\n",
        "  return merged_df\n",
        "\n",
        "def format_input(df, row, n_context=3):\n",
        "  past_pairs = df[\n",
        "    (df['participant_id'] == row['participant_id']) &\n",
        "    (df.index < row.name)\n",
        "  ].tail(n_context)\n",
        "\n",
        "  context_lines = []\n",
        "  for _, past_row in past_pairs.iterrows():\n",
        "    q = str(past_row.get(\"question\", \"\")).strip()\n",
        "    a = str(past_row.get(\"answer\", \"\")).strip()\n",
        "    context_lines.append(f\"Q: {q}\\nA: {a}\")\n",
        "\n",
        "  context = \"[START]\\n\" + \"\\n\".join(context_lines) if context_lines else \"[START]\\n\"\n",
        "\n",
        "  instruction = (\n",
        "    \"### Instruction:\\n\"\n",
        "    \"You are analyzing a therapeutic interview between a virtual interviewer (Ellie) and a participant.\\n\"\n",
        "    \"The participant has a PHQ-8 score ranging from 0 (no depression) to 24 (severe depression). \"\n",
        "    f\"This participant’s score is {row['depression_severity']}. \"\n",
        "    \"Scores of 10 or higher are typically considered indicative of depression.\\n\"\n",
        "    \"Given the participant’s previous responses and their PHQ score, \"\n",
        "    \"predict how they might answer the next question in a coherent and realistic way. \"\n",
        "    \"Use natural, casual language. Avoid overly formal styles. \"\n",
        "    \"Tolerate some irregularities (omissions, repetitions, filler words).\\n\\n\"\n",
        "  )\n",
        "\n",
        "  question = str(row.get(\"question\", \"\")).strip()\n",
        "\n",
        "  input_text = f\"### Input:\\n{context}\\nQ: {question}\\nA:\"\n",
        "\n",
        "  return instruction + input_text\n",
        "\n",
        "class InstructionDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
        "        dataframe = dataframe.sort_values(\n",
        "            by=['participant_id', 'start_time']\n",
        "        ).reset_index(drop=True)\n",
        "\n",
        "        self.df = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.samples = []\n",
        "\n",
        "        for _, row in self.df.iterrows():\n",
        "            prompt = format_input(self.df, row)\n",
        "            response = str(row.get(\"answer\", \"\")).strip()\n",
        "\n",
        "            full = f\"{prompt}\\n\\n### Response:\\n{response} [END]\"\n",
        "\n",
        "            self.samples.append((prompt, full))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        prompt, full_text = self.samples[idx]\n",
        "\n",
        "        encoded_full = self.tokenizer(\n",
        "            full_text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        encoded_prompt = self.tokenizer(\n",
        "            prompt,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_ids = encoded_full[\"input_ids\"].squeeze(0)\n",
        "        attention_mask = encoded_full[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        prompt_len = (encoded_prompt[\"input_ids\"] != self.tokenizer.pad_token_id).sum()\n",
        "\n",
        "        labels = input_ids.clone()\n",
        "        labels[:prompt_len] = -100  # ignore instruction & input tokens\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"labels\": labels,\n",
        "            \"attention_mask\": attention_mask\n",
        "        }\n",
        "\n",
        "def load_daic_data(tokenizer, data_dir=\"./daic_data/\", should_create_csv=False):\n",
        "  transcripts_dir = os.path.join(data_dir, \"transcripts\")\n",
        "  labels_dir = os.path.join(data_dir, \"labels\")\n",
        "\n",
        "  qa_df = get_questions_answers_df(transcripts_dir)\n",
        "  qa_df = add_labels_to_df(qa_df, labels_dir)\n",
        "\n",
        "  if should_create_csv:\n",
        "    qa_df.to_csv(\"questions_and_answers.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "  instruction_dataset = InstructionDataset(qa_df, tokenizer)\n",
        "  return instruction_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ti-ju1PKbEyH"
      },
      "outputs": [],
      "source": [
        "def get_tokenizer_and_early_model(model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        load_in_8bit=False,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    return tokenizer, model, model_name\n",
        "\n",
        "def get_lora_model(model):\n",
        "  lora_config = LoraConfig(\n",
        "    r=8, # rank\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"], # depends on model architecture\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        "  )\n",
        "\n",
        "  model = get_peft_model(model, lora_config)\n",
        "  model.print_trainable_parameters()\n",
        "  return model\n",
        "\n",
        "def fine_tune_model(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    tokenized_datasets,\n",
        "    output_dir=\"./tiny_llama_instruction_tuned\",\n",
        "):\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=10,\n",
        "        # num_train_epochs=1,\n",
        "        max_steps=100,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=10,\n",
        "        save_steps=444\n",
        "    )\n",
        "\n",
        "    def collator(batch):\n",
        "        return {\n",
        "            \"input_ids\": torch.stack([x[\"input_ids\"] for x in batch]),\n",
        "            \"labels\": torch.stack([x[\"labels\"] for x in batch]),\n",
        "            \"attention_mask\": torch.stack([x[\"attention_mask\"] for x in batch]),\n",
        "        }\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_datasets,\n",
        "        data_collator=collator,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "def use_tokenizer(tokenizer, text):\n",
        "  return tokenizer(text, truncation=True, padding='max_length', max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "ZRzYCDDqbcM6",
        "outputId": "b20f9452-3fa1-4de3-c966-ab5eade47889"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer and model...\n",
            "Loading dataset...\n",
            "Getting LoRA model...\n",
            "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n",
            "Fine-tuning model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 02:47, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.216800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.530400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.992100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.098700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.807300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.900400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.877600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.638500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.720800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.820400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print('Loading tokenizer and model...')\n",
        "tokenizer, model, model_name = get_tokenizer_and_early_model()\n",
        "\n",
        "print('Loading dataset...')\n",
        "tokenized_dataset = load_daic_data(tokenizer, should_create_csv=False)\n",
        "\n",
        "print('Getting LoRA model...')\n",
        "model = get_lora_model(model)\n",
        "\n",
        "print('Fine-tuning model...')\n",
        "fine_tune_model(model, tokenizer, tokenized_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKS6xfhB1qkV"
      },
      "source": [
        "### Analyzing models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "request = \"\"\"\n",
        "You are analyzing a therapeutic interview between a virtual interviewer (Ellie) and a participant.\n",
        "The participant has a PHQ-8 score ranging from 0 (no depression) to 24 (severe depression). This participant’s score is 3. Scores of 10 or higher are typically considered indicative of depression.\n",
        "Given the participant’s previous responses and their PHQ score, predict how they might answer the next question in a coherent and realistic way.Use natural, casual language. Avoid overly formal styles.Tolerate some irregularities (omissions, repetitions, filler words) given the conversational context.\n",
        "\n",
        "### Input:\n",
        "[START]\n",
        "Q: right there are always trade offs in life aren't there\n",
        "A: yeah\n",
        "Q: what made you decide to do that\n",
        "A: so um i think i think my in my life i knew that i there's a lot of things i have <ha> there's more dislikes <laughter> than likes so i kinda narrowed it down to what am i good at and what am i not good at and what am i gonna work well or who who am i gonna work well with and who will i not work well with so i kind of i kinda sorted out and then the list kind of mmm kind of answered itself so\n",
        "Q: that sounds really hard\n",
        "A: no it i don't think it was hard but it was just but i think it was a real reality check and i think it it's kind of a good thing 'cause sometimes trying to conform to doing things that doesn't really fit you doesn't make sense it's like trying to shove a a round peg into a square a square hole and it's like it just no matter how you try to shove it in it's not gonna go in so sometimes it's just might as well go down a path that seems to work better for you\n",
        "Q: right that makes sense what's one of your most memorable experiences\n",
        "\n",
        "### Response:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Last model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "629sz-qTpTVK",
        "outputId": "aa1ba84a-b270-4917-ae1f-e5d2a38d2bb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading best model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "You are analyzing a therapeutic interview between a virtual interviewer (Ellie) and a participant.\n",
            "The participant has a PHQ-8 score ranging from 0 (no depression) to 24 (severe depression). This participant’s score is 3. Scores of 10 or higher are typically considered indicative of depression.\n",
            "Given the participant’s previous responses and their PHQ score, predict how they might answer the next question in a coherent and realistic way.Use natural, casual language. Avoid overly formal styles.Tolerate some irregularities (omissions, repetitions, filler words) given the conversational context.\n",
            "\n",
            "### Input:\n",
            "[START]\n",
            "Q: right there are always trade offs in life aren't there\n",
            "A: yeah\n",
            "Q: what made you decide to do that\n",
            "A: so um i think i think my in my life i knew that i there's a lot of things i have <ha> there's more dislikes <laughter> than likes so i kinda narrowed it down to what am i good at and what am i not good at and what am i gonna work well or who who am i gonna work well with and who will i not work well with so i kind of i kinda sorted out and then the list kind of mmm kind of answered itself so\n",
            "Q: that sounds really hard\n",
            "A: no it i don't think it was hard but it was just but i think it was a real reality check and i think it it's kind of a good thing 'cause sometimes trying to conform to doing things that doesn't really fit you doesn't make sense it's like trying to shove a a round peg into a square a square hole and it's like it just no matter how you try to shove it in it's not gonna go in so sometimes it's just might as well go down a path that seems to work better for you\n",
            "Q: right that makes sense what's one of your most memorable experiences\n",
            "\n",
            "### Response:\n",
            "um the one that always comes to mind is the <laughter> it's um well it's a long time ago now [END] but i was a 16 year old girl going to a high school in california and the school had a lot of problems <laughter> and the principal had been fired and he was stealing money and the police were on the loose and the police were shooting at us and all this stuff and there's a lot of\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading best model...\")\n",
        "base = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
        "lora = PeftModel.from_pretrained(base, \"./tiny_llama_instruction_tuned\")\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=lora, tokenizer=tokenizer)\n",
        "\n",
        "res = pipe(request, max_new_tokens=100)\n",
        "output = res[0][\"generated_text\"]\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all checkpoint folders\n",
        "checkpoint_folders = [f for f in os.listdir(\"./tiny_llama_instruction_tuned\") if f.startswith(\"checkpoint-\")]\n",
        "\n",
        "# Load each checkpoint and evaluate\n",
        "for folder in checkpoint_folders:\n",
        "    lora = PeftModel.from_pretrained(base, f\"./tiny_llama_instruction_tuned/{folder}\")\n",
        "    pipe = pipeline(\"text-generation\", model=lora, tokenizer=tokenizer)\n",
        "\n",
        "    res = pipe(request, max_new_tokens=100)\n",
        "    output = res[0][\"generated_text\"]\n",
        "    \n",
        "    print(f\"Checkpoint {folder}: {output}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
