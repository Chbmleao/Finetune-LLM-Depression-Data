{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMaAhfOrazZp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import glob, os\n",
        "import pandas as pd\n",
        "import gc\n",
        "from transformers import pipeline\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "from transformers import TrainingArguments, Trainer, StoppingCriteria, StoppingCriteriaList\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/daic_data/daic_data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2AGaCQexbCS"
      },
      "source": [
        "## Data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Php58L8LbQNv"
      },
      "outputs": [],
      "source": [
        "def get_questions_answers_df(transcripts_dir):\n",
        "  transcripts_files = glob.glob(os.path.join(transcripts_dir, \"*.csv\"))\n",
        "\n",
        "  # Load and concatenate all transcript files\n",
        "  df = pd.concat(\n",
        "    (\n",
        "      pd.read_csv(file, sep=\"\\t\", encoding=\"utf-8-sig\").assign(source=os.path.basename(file))\n",
        "      for file in transcripts_files\n",
        "    ),\n",
        "    ignore_index=True\n",
        "  )\n",
        "\n",
        "  # Create block_id to identify contiguous speaker segments\n",
        "  df['block_id'] = (df['speaker'] != df['speaker'].shift(1)).cumsum()\n",
        "\n",
        "  # Aggregate by source and block_id to merge contiguous segments by the same speaker\n",
        "  df = df.groupby(['source', 'block_id']).agg(\n",
        "    speaker=('speaker', 'first'),\n",
        "    start_time=('start_time', 'min'),\n",
        "    stop_time=('stop_time', 'max'),\n",
        "    value=('value', lambda x: ' '.join(x.astype(str)))\n",
        "  )\n",
        "\n",
        "  # Sort by participant and time\n",
        "  df = df.sort_values(by=['source', 'start_time']).reset_index()\n",
        "\n",
        "  # Add previous speaker and value columns only if the previous source is the same\n",
        "  df['prev_speaker'] = df.groupby('source')['speaker'].shift(1)\n",
        "  df['prev_value'] = df.groupby('source')['value'].shift(1)\n",
        "\n",
        "  is_answer = (\n",
        "    (df['speaker'] == 'Participant') &\n",
        "    (df['prev_speaker'] == 'Ellie') &\n",
        "    (df['source'] == df['source'].shift(1))\n",
        "  )\n",
        "\n",
        "  df = df[is_answer].copy()\n",
        "  df = df.rename(columns={\n",
        "    'prev_value': 'question', # The previous Ellie utterance is the question\n",
        "    'value': 'answer',            # The current Participant utterance is the answer\n",
        "  })\n",
        "\n",
        "  df['participant_id'] = df['source'].str.split(\"_\").str[0].astype(int)\n",
        "  df = df[['participant_id', 'question', 'answer', 'start_time']]\n",
        "\n",
        "  return df\n",
        "\n",
        "def add_labels_to_df(qa_df, labels_dir):\n",
        "  splits = ['train', 'dev', 'test']\n",
        "\n",
        "  all_labels_df = pd.DataFrame()\n",
        "  for split in splits:\n",
        "    split_labels_df = pd.read_csv(os.path.join(labels_dir, f\"{split}.csv\"))\n",
        "    split_labels_df = split_labels_df.rename(columns={\n",
        "      \"Participant_ID\": \"participant_id\",\n",
        "      \"PHQ8_Binary\": \"depression_label\",\n",
        "      \"PHQ8_Score\": \"depression_severity\",\n",
        "      \"PHQ_Binary\": \"depression_label\",\n",
        "      \"PHQ_Score\": \"depression_severity\",\n",
        "    })\n",
        "    split_labels_df = split_labels_df[[\"participant_id\", \"depression_label\", \"depression_severity\"]]\n",
        "    split_labels_df[\"split\"] = split\n",
        "    all_labels_df = pd.concat([all_labels_df, split_labels_df], ignore_index=True)\n",
        "\n",
        "  merged_df = pd.merge(qa_df, all_labels_df, on=\"participant_id\", how=\"left\")\n",
        "  return merged_df\n",
        "\n",
        "def format_input(df, row, n_context=3):\n",
        "  past_pairs = df[\n",
        "    (df['participant_id'] == row['participant_id']) &\n",
        "    (df.index < row.name)\n",
        "  ].tail(n_context)\n",
        "\n",
        "  context_lines = []\n",
        "  for _, past_row in past_pairs.iterrows():\n",
        "    q = str(past_row.get(\"question\", \"\")).strip()\n",
        "    a = str(past_row.get(\"answer\", \"\")).strip()\n",
        "    context_lines.append(f\"Q: {q}\\nA: {a}\")\n",
        "\n",
        "  context = \"[START]\\n\" + \"\\n\".join(context_lines) if context_lines else \"[START]\\n\"\n",
        "\n",
        "  instruction = (\n",
        "    \"### Instruction:\\n\"\n",
        "    \"You are analyzing a therapeutic interview between a virtual interviewer (Ellie) and a participant.\\n\"\n",
        "    \"The participant has a PHQ-8 score ranging from 0 (no depression) to 24 (severe depression). \"\n",
        "    f\"This participant’s score is {row['depression_severity']}. \"\n",
        "    \"Scores of 10 or higher are typically considered indicative of depression.\\n\"\n",
        "    \"Given the participant’s previous responses and their PHQ score, \"\n",
        "    \"predict how they might answer the next question in a coherent and realistic way. \"\n",
        "    \"Use natural, casual language. Avoid overly formal styles. \"\n",
        "    \"Tolerate some irregularities (omissions, repetitions, filler words).\\n\\n\"\n",
        "  )\n",
        "\n",
        "  question = str(row.get(\"question\", \"\")).strip()\n",
        "\n",
        "  input_text = f\"### Input:\\n{context}\\nQ: {question}\\nA:\"\n",
        "\n",
        "  return instruction + input_text\n",
        "\n",
        "class InstructionDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
        "        dataframe = dataframe.sort_values(\n",
        "            by=['participant_id', 'start_time']\n",
        "        ).reset_index(drop=True)\n",
        "\n",
        "        self.df = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.samples = []\n",
        "\n",
        "        for _, row in self.df.iterrows():\n",
        "            prompt = format_input(self.df, row)\n",
        "            response = str(row.get(\"answer\", \"\")).strip()\n",
        "\n",
        "            full = f\"{prompt}\\n\\n### Response:\\n{response} [END]\"\n",
        "\n",
        "            self.samples.append((prompt, full))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        prompt, full_text = self.samples[idx]\n",
        "\n",
        "        encoded_full = self.tokenizer(\n",
        "            full_text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        encoded_prompt = self.tokenizer(\n",
        "            prompt,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_ids = encoded_full[\"input_ids\"].squeeze(0)\n",
        "        attention_mask = encoded_full[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        prompt_len = (encoded_prompt[\"input_ids\"] != self.tokenizer.pad_token_id).sum()\n",
        "\n",
        "        labels = input_ids.clone()\n",
        "        labels[:prompt_len] = -100  # ignore instruction & input tokens\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"labels\": labels,\n",
        "            \"attention_mask\": attention_mask\n",
        "        }\n",
        "\n",
        "def load_daic_data(tokenizer, data_dir=\"./daic_data/\", should_create_csv=False, return_splits=False):\n",
        "  \"\"\"Load DAIC data and optionally return train/validation splits.\n",
        "  \n",
        "  Args:\n",
        "    tokenizer: Tokenizer instance\n",
        "    data_dir: Directory containing data\n",
        "    should_create_csv: Whether to save CSV file\n",
        "    return_splits: If True, return train and validation datasets separately\n",
        "    \n",
        "  Returns:\n",
        "    If return_splits=False: single InstructionDataset with all data\n",
        "    If return_splits=True: tuple of (train_dataset, val_dataset)\n",
        "  \"\"\"\n",
        "  transcripts_dir = os.path.join(data_dir, \"transcripts\")\n",
        "  labels_dir = os.path.join(data_dir, \"labels\")\n",
        "\n",
        "  qa_df = get_questions_answers_df(transcripts_dir)\n",
        "  qa_df = add_labels_to_df(qa_df, labels_dir)\n",
        "\n",
        "  if should_create_csv:\n",
        "    qa_df.to_csv(\"questions_and_answers.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "  if return_splits:\n",
        "    # Split into train and validation (using 'dev' as validation)\n",
        "    train_df = qa_df[qa_df['split'] == 'train'].copy()\n",
        "    val_df = qa_df[qa_df['split'] == 'dev'].copy()\n",
        "    \n",
        "    train_dataset = InstructionDataset(train_df, tokenizer)\n",
        "    val_dataset = InstructionDataset(val_df, tokenizer)\n",
        "    \n",
        "    print(f\"Train samples: {len(train_dataset)}\")\n",
        "    print(f\"Validation samples: {len(val_dataset)}\")\n",
        "    \n",
        "    return train_dataset, val_dataset\n",
        "  else:\n",
        "    instruction_dataset = InstructionDataset(qa_df, tokenizer)\n",
        "    return instruction_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ti-ju1PKbEyH"
      },
      "outputs": [],
      "source": [
        "def get_tokenizer_and_early_model(model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Add [END] as a special token\n",
        "    special_tokens_dict = {\"additional_special_tokens\": [\"[END]\"]}\n",
        "    tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        load_in_8bit=False,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Resize token embeddings to accommodate the new [END] token\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    return tokenizer, model, model_name\n",
        "\n",
        "def get_lora_model(model):\n",
        "  lora_config = LoraConfig(\n",
        "    r=8, # rank\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"], # depends on model architecture\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        "  )\n",
        "\n",
        "  model = get_peft_model(model, lora_config)\n",
        "  model.print_trainable_parameters()\n",
        "  return model\n",
        "\n",
        "def fine_tune_model(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    train_dataset,\n",
        "    output_dir=\"./tiny_llama_instruction_tuned\",\n",
        "    eval_dataset=None,\n",
        "):\n",
        "    \"\"\"Fine-tune model with optional validation dataset for monitoring.\n",
        "    \n",
        "    Args:\n",
        "        model: Model to fine-tune\n",
        "        tokenizer: Tokenizer instance\n",
        "        train_dataset: Training dataset\n",
        "        output_dir: Output directory for checkpoints\n",
        "        eval_dataset: Optional validation dataset for monitoring\n",
        "    \"\"\"\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_ratio=0.1,  # 10% of training steps for warmup (scales with dataset size)\n",
        "        num_train_epochs=1,  # Train on entire dataset (1333 steps)\n",
        "        learning_rate=1e-4,  # Reduced from 2e-4 for more gradual learning\n",
        "        lr_scheduler_type=\"cosine\",  # Cosine decay for smooth learning rate reduction\n",
        "        fp16=True,\n",
        "        logging_steps=50,  # Increased since we have more steps now\n",
        "        save_steps=200,  # Save checkpoints less frequently (every ~200 steps)\n",
        "        eval_strategy=\"steps\" if eval_dataset else \"no\",  # Evaluate during training if val set provided\n",
        "        eval_steps=200,  # Evaluate every 200 steps (same as save_steps)\n",
        "        save_total_limit=3,  # Keep only last 3 checkpoints to save space\n",
        "        load_best_model_at_end=True if eval_dataset else False,  # Load best model if validation set provided\n",
        "        metric_for_best_model=\"eval_loss\" if eval_dataset else None,\n",
        "        greater_is_better=False,  # Lower loss is better\n",
        "    )\n",
        "\n",
        "    def collator(batch):\n",
        "        return {\n",
        "            \"input_ids\": torch.stack([x[\"input_ids\"] for x in batch]),\n",
        "            \"labels\": torch.stack([x[\"labels\"] for x in batch]),\n",
        "            \"attention_mask\": torch.stack([x[\"attention_mask\"] for x in batch]),\n",
        "        }\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        data_collator=collator,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "def use_tokenizer(tokenizer, text):\n",
        "  return tokenizer(text, truncation=True, padding='max_length', max_length=512)\n",
        "\n",
        "# Stopping criteria for [END] token\n",
        "class EndTokenStoppingCriteria(StoppingCriteria):\n",
        "    def __init__(self, end_token_id):\n",
        "        self.end_token_id = end_token_id\n",
        "    \n",
        "    def __call__(self, input_ids, scores, **kwargs):\n",
        "        # Stop if the last generated token is [END]\n",
        "        return input_ids[0][-1] == self.end_token_id\n",
        "\n",
        "def create_stopping_criteria(tokenizer):\n",
        "    \"\"\"Create stopping criteria that stops at [END] token.\"\"\"\n",
        "    end_token_id = tokenizer.convert_tokens_to_ids(\"[END]\")\n",
        "    return StoppingCriteriaList([EndTokenStoppingCriteria(end_token_id)])\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Clear GPU and CPU memory cache.\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "def load_finetuned_model(model_name, tokenizer, checkpoint_path=None):\n",
        "    \"\"\"Load a finetuned model from checkpoint or final model.\n",
        "    Note: Base model is loaded fresh each time to avoid PEFT weight conflicts.\n",
        "    \n",
        "    Args:\n",
        "        model_name: Base model name\n",
        "        tokenizer: Tokenizer instance\n",
        "        checkpoint_path: Path to checkpoint, or None for final model\n",
        "    \"\"\"\n",
        "    # Always load base model fresh to avoid PEFT weight conflicts\n",
        "    # (PEFT models modify base model in place, so we can't reuse it)\n",
        "    base = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
        "    \n",
        "    # Resize token embeddings if [END] token was added\n",
        "    if len(tokenizer) != base.get_input_embeddings().weight.shape[0]:\n",
        "        base.resize_token_embeddings(len(tokenizer))\n",
        "    \n",
        "    if checkpoint_path:\n",
        "        lora = PeftModel.from_pretrained(base, checkpoint_path)\n",
        "    else:\n",
        "        lora = PeftModel.from_pretrained(base, \"./tiny_llama_instruction_tuned\")\n",
        "    \n",
        "    return lora, base  # Return both so we can clean up base separately\n",
        "\n",
        "def generate_response(model, tokenizer, prompt, max_new_tokens=100, stopping_criteria=None):\n",
        "    \"\"\"Generate a response using the model. Cleans up pipeline after use.\"\"\"\n",
        "    pipe = None\n",
        "    try:\n",
        "        pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "        \n",
        "        if stopping_criteria is None:\n",
        "            stopping_criteria = create_stopping_criteria(tokenizer)\n",
        "        \n",
        "        res = pipe(prompt, max_new_tokens=max_new_tokens, stopping_criteria=stopping_criteria)\n",
        "        return res[0][\"generated_text\"]\n",
        "    finally:\n",
        "        # Clean up pipeline\n",
        "        if pipe is not None:\n",
        "            del pipe\n",
        "        clear_memory()\n",
        "\n",
        "def extract_response_only(full_output, prompt):\n",
        "    \"\"\"Extract only the generated response part, removing the prompt.\"\"\"\n",
        "    if full_output.startswith(prompt):\n",
        "        return full_output[len(prompt):].strip()\n",
        "    return full_output\n",
        "\n",
        "def unload_model(model):\n",
        "    \"\"\"Unload a model from memory.\"\"\"\n",
        "    if model is not None:\n",
        "        # Move to CPU and delete\n",
        "        if hasattr(model, 'cpu'):\n",
        "            model.cpu()\n",
        "        del model\n",
        "        clear_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "ZRzYCDDqbcM6",
        "outputId": "b20f9452-3fa1-4de3-c966-ab5eade47889"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer and model...\n",
            "Loading dataset...\n",
            "Getting LoRA model...\n",
            "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n",
            "Fine-tuning model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 02:47, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.216800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.530400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.992100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.098700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.807300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.900400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.877600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.638500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.720800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.820400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print('Loading tokenizer and model...')\n",
        "tokenizer, model, model_name = get_tokenizer_and_early_model()\n",
        "\n",
        "print('Loading datasets (train and validation)...')\n",
        "train_dataset, val_dataset = load_daic_data(tokenizer, should_create_csv=False, return_splits=True)\n",
        "\n",
        "print('Getting LoRA model...')\n",
        "model = get_lora_model(model)\n",
        "\n",
        "print('Fine-tuning model with validation monitoring...')\n",
        "fine_tune_model(model, tokenizer, train_dataset, eval_dataset=val_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKS6xfhB1qkV"
      },
      "source": [
        "### Analyzing models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "request = \"\"\"\n",
        "You are analyzing a therapeutic interview between a virtual interviewer (Ellie) and a participant.\n",
        "The participant has a PHQ-8 score ranging from 0 (no depression) to 24 (severe depression). This participant’s score is 3. Scores of 10 or higher are typically considered indicative of depression.\n",
        "Given the participant’s previous responses and their PHQ score, predict how they might answer the next question in a coherent and realistic way.Use natural, casual language. Avoid overly formal styles.Tolerate some irregularities (omissions, repetitions, filler words) given the conversational context.\n",
        "\n",
        "### Input:\n",
        "[START]\n",
        "Q: right there are always trade offs in life aren't there\n",
        "A: yeah\n",
        "Q: what made you decide to do that\n",
        "A: so um i think i think my in my life i knew that i there's a lot of things i have <ha> there's more dislikes <laughter> than likes so i kinda narrowed it down to what am i good at and what am i not good at and what am i gonna work well or who who am i gonna work well with and who will i not work well with so i kind of i kinda sorted out and then the list kind of mmm kind of answered itself so\n",
        "Q: that sounds really hard\n",
        "A: no it i don't think it was hard but it was just but i think it was a real reality check and i think it it's kind of a good thing 'cause sometimes trying to conform to doing things that doesn't really fit you doesn't make sense it's like trying to shove a a round peg into a square a square hole and it's like it just no matter how you try to shove it in it's not gonna go in so sometimes it's just might as well go down a path that seems to work better for you\n",
        "Q: right that makes sense what's one of your most memorable experiences\n",
        "\n",
        "### Response:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Last model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "629sz-qTpTVK",
        "outputId": "aa1ba84a-b270-4917-ae1f-e5d2a38d2bb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading best model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "You are analyzing a therapeutic interview between a virtual interviewer (Ellie) and a participant.\n",
            "The participant has a PHQ-8 score ranging from 0 (no depression) to 24 (severe depression). This participant’s score is 3. Scores of 10 or higher are typically considered indicative of depression.\n",
            "Given the participant’s previous responses and their PHQ score, predict how they might answer the next question in a coherent and realistic way.Use natural, casual language. Avoid overly formal styles.Tolerate some irregularities (omissions, repetitions, filler words) given the conversational context.\n",
            "\n",
            "### Input:\n",
            "[START]\n",
            "Q: right there are always trade offs in life aren't there\n",
            "A: yeah\n",
            "Q: what made you decide to do that\n",
            "A: so um i think i think my in my life i knew that i there's a lot of things i have <ha> there's more dislikes <laughter> than likes so i kinda narrowed it down to what am i good at and what am i not good at and what am i gonna work well or who who am i gonna work well with and who will i not work well with so i kind of i kinda sorted out and then the list kind of mmm kind of answered itself so\n",
            "Q: that sounds really hard\n",
            "A: no it i don't think it was hard but it was just but i think it was a real reality check and i think it it's kind of a good thing 'cause sometimes trying to conform to doing things that doesn't really fit you doesn't make sense it's like trying to shove a a round peg into a square a square hole and it's like it just no matter how you try to shove it in it's not gonna go in so sometimes it's just might as well go down a path that seems to work better for you\n",
            "Q: right that makes sense what's one of your most memorable experiences\n",
            "\n",
            "### Response:\n",
            "um the one that always comes to mind is the <laughter> it's um well it's a long time ago now [END] but i was a 16 year old girl going to a high school in california and the school had a lot of problems <laughter> and the principal had been fired and he was stealing money and the police were on the loose and the police were shooting at us and all this stuff and there's a lot of\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading final model...\")\n",
        "model, base_model = load_finetuned_model(model_name, tokenizer)\n",
        "\n",
        "try:\n",
        "    print(\"\\nGenerating response...\")\n",
        "    full_output = generate_response(model, tokenizer, request, max_new_tokens=100)\n",
        "    response_only = extract_response_only(full_output, request)\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"FULL OUTPUT:\")\n",
        "    print(\"=\" * 80)\n",
        "    print(full_output)\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"RESPONSE ONLY:\")\n",
        "    print(\"=\" * 80)\n",
        "    print(response_only)\n",
        "finally:\n",
        "    # Clean up models from memory\n",
        "    unload_model(model)\n",
        "    unload_model(base_model)\n",
        "    print(\"\\nModels unloaded from memory.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_all_checkpoints(model_name, tokenizer, prompt, output_dir=\"./tiny_llama_instruction_tuned\"):\n",
        "    \"\"\"Test all checkpoint models and return results. Memory-efficient version.\n",
        "    Each model is loaded, tested, and immediately unloaded to save memory.\n",
        "    \n",
        "    Args:\n",
        "        model_name: Base model name\n",
        "        tokenizer: Tokenizer instance\n",
        "        prompt: Prompt to test with\n",
        "        output_dir: Directory containing checkpoints\n",
        "    \"\"\"\n",
        "    checkpoint_folders = sorted([\n",
        "        f for f in os.listdir(output_dir) \n",
        "        if f.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(output_dir, f))\n",
        "    ])\n",
        "    \n",
        "    if not checkpoint_folders:\n",
        "        print(\"No checkpoints found.\")\n",
        "        return {}\n",
        "    \n",
        "    results = {}\n",
        "    stopping_criteria = create_stopping_criteria(tokenizer)\n",
        "    \n",
        "    print(f\"Found {len(checkpoint_folders)} checkpoints. Testing each (memory-efficient mode)...\\n\")\n",
        "    \n",
        "    for i, folder in enumerate(checkpoint_folders, 1):\n",
        "        checkpoint_path = os.path.join(output_dir, folder)\n",
        "        print(f\"[{i}/{len(checkpoint_folders)}] Testing {folder}...\")\n",
        "        \n",
        "        model = None\n",
        "        base_model = None\n",
        "        try:\n",
        "            # Load model (returns both lora and base for cleanup)\n",
        "            model, base_model = load_finetuned_model(model_name, tokenizer, checkpoint_path)\n",
        "            \n",
        "            # Generate response\n",
        "            full_output = generate_response(model, tokenizer, prompt, max_new_tokens=100, stopping_criteria=stopping_criteria)\n",
        "            response_only = extract_response_only(full_output, prompt)\n",
        "            \n",
        "            results[folder] = {\n",
        "                \"full_output\": full_output,\n",
        "                \"response_only\": response_only\n",
        "            }\n",
        "            \n",
        "            print(f\"✓ {folder} completed\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error testing {folder}: {e}\")\n",
        "            results[folder] = {\"error\": str(e)}\n",
        "        finally:\n",
        "            # Always unload models after each checkpoint to free memory\n",
        "            if model is not None:\n",
        "                unload_model(model)\n",
        "            if base_model is not None:\n",
        "                unload_model(base_model)\n",
        "            print(f\"  Memory freed after {folder}\\n\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Test all checkpoints\n",
        "checkpoint_results = test_all_checkpoints(model_name, tokenizer, request)\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CHECKPOINT COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "for checkpoint_name, result in checkpoint_results.items():\n",
        "    if \"error\" in result:\n",
        "        print(f\"\\n{checkpoint_name}: ERROR - {result['error']}\")\n",
        "    else:\n",
        "        print(f\"\\n{checkpoint_name}:\")\n",
        "        print(\"-\" * 80)\n",
        "        print(result[\"response_only\"])\n",
        "        print()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
