{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd33978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob, os\n",
    "import pandas as pd\n",
    "import gc\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# Set CUDA memory allocation to reduce fragmentation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Clear CUDA cache at startup\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    print(f\"CUDA available. GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab923406",
   "metadata": {},
   "source": [
    "# Dataset Augmentation\n",
    "\n",
    "This notebook loads a finetuned model and generates augmented answers for each question in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f55f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions from daic_finetune.ipynb\n",
    "\n",
    "def get_questions_answers_df(transcripts_dir):\n",
    "  transcripts_files = glob.glob(os.path.join(transcripts_dir, \"*.csv\"))\n",
    "\n",
    "  # Load and concatenate all transcript files\n",
    "  df = pd.concat(\n",
    "    (\n",
    "      pd.read_csv(file, sep=\"\\t\", encoding=\"utf-8-sig\").assign(source=os.path.basename(file))\n",
    "      for file in transcripts_files\n",
    "    ),\n",
    "    ignore_index=True\n",
    "  )\n",
    "\n",
    "  # Create block_id to identify contiguous speaker segments\n",
    "  df['block_id'] = (df['speaker'] != df['speaker'].shift(1)).cumsum()\n",
    "\n",
    "  # Aggregate by source and block_id to merge contiguous segments by the same speaker\n",
    "  df = df.groupby(['source', 'block_id']).agg(\n",
    "    speaker=('speaker', 'first'),\n",
    "    start_time=('start_time', 'min'),\n",
    "    stop_time=('stop_time', 'max'),\n",
    "    value=('value', lambda x: ' '.join(x.astype(str)))\n",
    "  )\n",
    "\n",
    "  # Sort by participant and time\n",
    "  df = df.sort_values(by=['source', 'start_time']).reset_index()\n",
    "\n",
    "  # Add previous speaker and value columns only if the previous source is the same\n",
    "  df['prev_speaker'] = df.groupby('source')['speaker'].shift(1)\n",
    "  df['prev_value'] = df.groupby('source')['value'].shift(1)\n",
    "\n",
    "  is_answer = (\n",
    "    (df['speaker'] == 'Participant') &\n",
    "    (df['prev_speaker'] == 'Ellie') &\n",
    "    (df['source'] == df['source'].shift(1))\n",
    "  )\n",
    "\n",
    "  df = df[is_answer].copy()\n",
    "  df = df.rename(columns={\n",
    "    'prev_value': 'question', # The previous Ellie utterance is the question\n",
    "    'value': 'answer',            # The current Participant utterance is the answer\n",
    "  })\n",
    "\n",
    "  df['participant_id'] = df['source'].str.split(\"_\").str[0].astype(int)\n",
    "  df = df[['participant_id', 'question', 'answer', 'start_time']]\n",
    "\n",
    "  return df\n",
    "\n",
    "def add_labels_to_df(qa_df, labels_dir):\n",
    "  splits = ['train', 'dev', 'test']\n",
    "\n",
    "  all_labels_df = pd.DataFrame()\n",
    "  for split in splits:\n",
    "    split_labels_df = pd.read_csv(os.path.join(labels_dir, f\"{split}.csv\"))\n",
    "    split_labels_df = split_labels_df.rename(columns={\n",
    "      \"Participant_ID\": \"participant_id\",\n",
    "      \"PHQ8_Binary\": \"depression_label\",\n",
    "      \"PHQ8_Score\": \"depression_severity\",\n",
    "      \"PHQ_Binary\": \"depression_label\",\n",
    "      \"PHQ_Score\": \"depression_severity\",\n",
    "    })\n",
    "    split_labels_df = split_labels_df[[\"participant_id\", \"depression_label\", \"depression_severity\"]]\n",
    "    split_labels_df[\"split\"] = split\n",
    "    all_labels_df = pd.concat([all_labels_df, split_labels_df], ignore_index=True)\n",
    "\n",
    "  merged_df = pd.merge(qa_df, all_labels_df, on=\"participant_id\", how=\"left\")\n",
    "  return merged_df\n",
    "\n",
    "def format_input(df, row, n_context=3):\n",
    "  past_pairs = df[\n",
    "    (df['participant_id'] == row['participant_id']) &\n",
    "    (df.index < row.name)\n",
    "  ].tail(n_context)\n",
    "\n",
    "  context_lines = []\n",
    "  for _, past_row in past_pairs.iterrows():\n",
    "    q = str(past_row.get(\"question\", \"\")).strip()\n",
    "    a = str(past_row.get(\"answer\", \"\")).strip()\n",
    "    context_lines.append(f\"Q: {q}\\nA: {a}\")\n",
    "\n",
    "  context = \"[START]\\n\" + \"\\n\".join(context_lines) if context_lines else \"[START]\\n\"\n",
    "\n",
    "  instruction = (\n",
    "    \"### Instruction:\\n\"\n",
    "    \"You are analyzing a therapeutic interview between a virtual interviewer (Ellie) and a participant.\\n\"\n",
    "    \"The participant has a PHQ-8 score ranging from 0 (no depression) to 24 (severe depression). \"\n",
    "    f\"This participant's score is {row['depression_severity']}. \"\n",
    "    \"Scores of 10 or higher are typically considered indicative of depression.\\n\"\n",
    "    \"Given the participant's previous responses and their PHQ score, \"\n",
    "    \"predict how they might answer the next question in a coherent and realistic way. \"\n",
    "    \"Use natural, casual language. Avoid overly formal styles. \"\n",
    "    \"Tolerate some irregularities (omissions, repetitions, filler words).\\n\\n\"\n",
    "  )\n",
    "\n",
    "  question = str(row.get(\"question\", \"\")).strip()\n",
    "\n",
    "  input_text = f\"### Input:\\n{context}\\nQ: {question}\\nA:\"\n",
    "\n",
    "  return instruction + input_text\n",
    "\n",
    "# Stopping criteria for [END] token\n",
    "class EndTokenStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, end_token_id):\n",
    "        self.end_token_id = end_token_id\n",
    "    \n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        # Stop if the last generated token is [END]\n",
    "        return input_ids[0][-1] == self.end_token_id\n",
    "\n",
    "def create_stopping_criteria(tokenizer):\n",
    "    \"\"\"Create stopping criteria that stops at [END] token.\"\"\"\n",
    "    end_token_id = tokenizer.convert_tokens_to_ids(\"[END]\")\n",
    "    return StoppingCriteriaList([EndTokenStoppingCriteria(end_token_id)])\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU and CPU memory cache.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def reset_cuda_memory():\n",
    "    \"\"\"Aggressively clear all CUDA memory.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "def load_finetuned_model(model_name, tokenizer, checkpoint_path=None):\n",
    "    \"\"\"Load a finetuned model from checkpoint or final model.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Base model name\n",
    "        tokenizer: Tokenizer instance (must have [END] token)\n",
    "        checkpoint_path: Path to checkpoint, or None for final model\n",
    "        \n",
    "    Returns:\n",
    "        Loaded PEFT model\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    \n",
    "    # Determine device\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Load base model - don't use device_map to avoid offloading issues\n",
    "    print(f\"Loading base model...\")\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=None,  # Don't use device_map to avoid offloading\n",
    "    )\n",
    "    \n",
    "    # Manually move to device\n",
    "    base = base.to(device)\n",
    "    \n",
    "    # Resize token embeddings if [END] token was added\n",
    "    vocab_size = len(tokenizer)\n",
    "    model_vocab_size = base.get_input_embeddings().weight.shape[0]\n",
    "    \n",
    "    if vocab_size != model_vocab_size:\n",
    "        print(f\"Resizing token embeddings from {model_vocab_size} to {vocab_size}...\")\n",
    "        base.resize_token_embeddings(vocab_size)\n",
    "    \n",
    "    # Load PEFT model - don't use device_map to avoid offloading issues\n",
    "    print(\"Loading PEFT adapter...\")\n",
    "    if checkpoint_path:\n",
    "        lora = PeftModel.from_pretrained(\n",
    "            base, \n",
    "            checkpoint_path,\n",
    "            device_map=None,  # Don't use device_map\n",
    "        )\n",
    "    else:\n",
    "        lora = PeftModel.from_pretrained(\n",
    "            base, \n",
    "            \"./tiny_llama_instruction_tuned\",\n",
    "            device_map=None,  # Don't use device_map\n",
    "        )\n",
    "    \n",
    "    # Ensure model is on the correct device and in eval mode\n",
    "    lora = lora.to(device)\n",
    "    lora.eval()\n",
    "    \n",
    "    return lora\n",
    "\n",
    "def generate_response(model, tokenizer, prompt, max_new_tokens=150, stopping_criteria=None):\n",
    "    \"\"\"Generate a response using the model.\"\"\"\n",
    "    pipe = None\n",
    "    try:\n",
    "        pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "        \n",
    "        if stopping_criteria is None:\n",
    "            stopping_criteria = create_stopping_criteria(tokenizer)\n",
    "        \n",
    "        res = pipe(prompt, max_new_tokens=max_new_tokens, stopping_criteria=stopping_criteria)\n",
    "        return res[0][\"generated_text\"]\n",
    "    finally:\n",
    "        # Clean up pipeline\n",
    "        if pipe is not None:\n",
    "            del pipe\n",
    "        clear_memory()\n",
    "\n",
    "def extract_response_only(full_output, prompt):\n",
    "    \"\"\"Extract only the generated response part, removing the prompt.\"\"\"\n",
    "    if full_output.startswith(prompt):\n",
    "        return full_output[len(prompt):].strip()\n",
    "    return full_output\n",
    "\n",
    "def clean_augmented_answer(answer):\n",
    "    \"\"\"Clean the augmented answer by removing [END] token and extra whitespace.\"\"\"\n",
    "    if answer:\n",
    "        # Remove [END] token if present\n",
    "        answer = answer.replace(\"[END]\", \"\").strip()\n",
    "        # Remove any trailing incomplete sentences or fragments\n",
    "        return answer\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff67c840",
   "metadata": {},
   "source": [
    "## Load Finetuned Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d34344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "checkpoint_path = None  # Set to checkpoint path if you want to use a specific checkpoint, else uses final model\n",
    "model_dir = \"./tiny_llama_instruction_tuned\"  # Directory where finetuned model is saved\n",
    "data_dir = \"./daic_data/\"\n",
    "output_csv_path = \"./augmented_dataset.csv\"\n",
    "\n",
    "# Clear memory before loading\n",
    "print(\"Clearing GPU memory...\")\n",
    "reset_cuda_memory()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    free = total - reserved\n",
    "    print(f\"GPU memory - Total: {total:.2f} GB, Reserved: {reserved:.2f} GB, Free: {free:.2f} GB\")\n",
    "\n",
    "print(\"\\nLoading tokenizer...\")\n",
    "# Try to load tokenizer from saved model directory first (has [END] token)\n",
    "if os.path.exists(model_dir) and os.path.exists(os.path.join(model_dir, \"tokenizer_config.json\")):\n",
    "    print(f\"Loading tokenizer from {model_dir}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "else:\n",
    "    print(f\"Loading tokenizer from {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Add [END] as a special token if not already present\n",
    "    if \"[END]\" not in tokenizer.get_vocab():\n",
    "        special_tokens_dict = {\"additional_special_tokens\": [\"[END]\"]}\n",
    "        tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "print(\"\\nLoading finetuned model...\")\n",
    "model = load_finetuned_model(model_name, tokenizer, checkpoint_path)\n",
    "\n",
    "# Check memory after loading\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    print(f\"\\nModel loaded. Memory - Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c2e634",
   "metadata": {},
   "source": [
    "## Load Original Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63808f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading original dataset...\")\n",
    "transcripts_dir = os.path.join(data_dir, \"transcripts\")\n",
    "labels_dir = os.path.join(data_dir, \"labels\")\n",
    "\n",
    "# Load questions and answers\n",
    "qa_df = get_questions_answers_df(transcripts_dir)\n",
    "qa_df = add_labels_to_df(qa_df, labels_dir)\n",
    "\n",
    "# Sort by participant and time to maintain interview order\n",
    "qa_df = qa_df.sort_values(by=['participant_id', 'start_time']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Total samples: {len(qa_df)}\")\n",
    "print(f\"Participants: {qa_df['participant_id'].nunique()}\")\n",
    "print(f\"\\nSplit distribution:\")\n",
    "print(qa_df['split'].value_counts())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(qa_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea339db",
   "metadata": {},
   "source": [
    "## Augment Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4723fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_dataset(df, model, tokenizer, progress_interval=50):\n",
    "    \"\"\"\n",
    "    Augment dataset by generating new answers for each question.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with questions and answers\n",
    "        model: Finetuned model for generation\n",
    "        tokenizer: Tokenizer instance\n",
    "        progress_interval: Print progress every N samples\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with original and augmented answers\n",
    "    \"\"\"\n",
    "    augmented_data = []\n",
    "    stopping_criteria = create_stopping_criteria(tokenizer)\n",
    "    \n",
    "    total_samples = len(df)\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    print(f\"Starting augmentation for {total_samples} samples...\\n\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            # Build prompt with context\n",
    "            prompt = format_input(df, row, n_context=3)\n",
    "            \n",
    "            # Generate augmented answer\n",
    "            full_output = generate_response(\n",
    "                model, \n",
    "                tokenizer, \n",
    "                prompt, \n",
    "                max_new_tokens=150, \n",
    "                stopping_criteria=stopping_criteria\n",
    "            )\n",
    "            \n",
    "            # Extract only the response\n",
    "            augmented_answer = extract_response_only(full_output, prompt)\n",
    "            augmented_answer = clean_augmented_answer(augmented_answer)\n",
    "            \n",
    "            # Store results\n",
    "            augmented_data.append({\n",
    "                'participant_id': row['participant_id'],\n",
    "                'question': row['question'],\n",
    "                'original_answer': row['answer'],\n",
    "                'augmented_answer': augmented_answer,\n",
    "                'depression_severity': row['depression_severity'],\n",
    "                'depression_label': row.get('depression_label', None),\n",
    "                'split': row.get('split', None),\n",
    "                'start_time': row['start_time'],\n",
    "            })\n",
    "            \n",
    "            successful += 1\n",
    "            \n",
    "            # Progress update\n",
    "            if (idx + 1) % progress_interval == 0:\n",
    "                print(f\"Progress: {idx + 1}/{total_samples} ({100*(idx+1)/total_samples:.1f}%) - \"\n",
    "                      f\"Successful: {successful}, Failed: {failed}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {idx + 1} (participant {row['participant_id']}): {e}\")\n",
    "            failed += 1\n",
    "            # Still add the row with empty augmented answer\n",
    "            augmented_data.append({\n",
    "                'participant_id': row['participant_id'],\n",
    "                'question': row['question'],\n",
    "                'original_answer': row['answer'],\n",
    "                'augmented_answer': '',  # Empty on error\n",
    "                'depression_severity': row['depression_severity'],\n",
    "                'depression_label': row.get('depression_label', None),\n",
    "                'split': row.get('split', None),\n",
    "                'start_time': row['start_time'],\n",
    "            })\n",
    "    \n",
    "    print(f\"\\nAugmentation complete!\")\n",
    "    print(f\"Total: {total_samples}, Successful: {successful}, Failed: {failed}\")\n",
    "    \n",
    "    return pd.DataFrame(augmented_data)\n",
    "\n",
    "# Run augmentation\n",
    "augmented_df = augment_dataset(qa_df, model, tokenizer, progress_interval=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd95ac1d",
   "metadata": {},
   "source": [
    "## Save Augmented Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632e1ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display statistics\n",
    "print(\"Augmented Dataset Statistics:\")\n",
    "print(f\"Total samples: {len(augmented_df)}\")\n",
    "print(f\"Non-empty augmented answers: {(augmented_df['augmented_answer'] != '').sum()}\")\n",
    "print(f\"Empty augmented answers: {(augmented_df['augmented_answer'] == '').sum()}\")\n",
    "\n",
    "print(\"\\nSample augmented responses:\")\n",
    "sample_df = augmented_df[augmented_df['augmented_answer'] != ''].head(3)\n",
    "for idx, row in sample_df.iterrows():\n",
    "    print(f\"\\n--- Sample {idx + 1} ---\")\n",
    "    print(f\"Question: {row['question'][:100]}...\")\n",
    "    print(f\"Original: {row['original_answer'][:100]}...\")\n",
    "    print(f\"Augmented: {row['augmented_answer'][:100]}...\")\n",
    "\n",
    "# Save to CSV\n",
    "print(f\"\\nSaving augmented dataset to {output_csv_path}...\")\n",
    "augmented_df.to_csv(output_csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Dataset saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4465106",
   "metadata": {},
   "source": [
    "## Optional: Preview Augmented Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065c7ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display full augmented dataset\n",
    "print(\"Augmented Dataset Preview:\")\n",
    "print(augmented_df.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
